{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import h5py\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential, Model, model_from_json\n",
    "from keras.layers import Dense, Activation\n",
    "from keras import optimizers, layers, callbacks\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import hamming_loss, coverage_error, average_precision_score\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelBinarizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from skmultilearn.adapt import MLkNN\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "whole = pd.read_csv ('C:/Users/z5023853/OneDrive - UNSW/Jupyter/Feature_selected/ac.csv')\n",
    "train, test = train_test_split (whole,test_size=0.2)\n",
    "train.to_csv (path_or_buf = 'C:/Users/z5023853/OneDrive - UNSW/Jupyter/Feature_selected/train_cat_ac.csv')\n",
    "test.to_csv (path_or_buf = 'C:/Users/z5023853/OneDrive - UNSW/Jupyter/Feature_selected/test_cat_ac.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training cost (IM1, IM2, IM3, OS1), (11,12,13,7)\n",
    "# ivis=46, iev=47, ilc=50, iqt=51, icost=52, icd=54, ipro=55, iothers=57\n",
    "# CS=58, BT=59, per=60, PT=61, Location=62, yr=63, contract=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train set\n",
    "x_o_train = np.genfromtxt ('C:/Users/z5023853/OneDrive - UNSW/Jupyter/Feature_selected/train_cat_ac.csv', \n",
    "                    delimiter = ',', usecols = (11,12,13,7),skip_header =2)\n",
    "y_o_train = np.genfromtxt ('C:/Users/z5023853/OneDrive - UNSW/Jupyter/Feature_selected/train_cat_ac.csv', \n",
    "                    delimiter = ',', usecols = (46,47,50,51,52,54,55,57),skip_header =2)\n",
    "CS_train = pd.read_csv ('C:/Users/z5023853/OneDrive - UNSW/Jupyter/Feature_selected/train_cat_ac.csv', \n",
    "                    names = [\"CS_cat\"], header = 1)\n",
    "BT_train = pd.read_csv ('C:/Users/z5023853/OneDrive - UNSW/Jupyter/Feature_selected/train_cat_ac.csv', \n",
    "                    names = [\"BT_cat\"], header = 1)\n",
    "per_train = pd.read_csv ('C:/Users/z5023853/OneDrive - UNSW/Jupyter/Feature_selected/train_cat_ac.csv', \n",
    "                    names = [\"per_cat\"], header = 1)\n",
    "PT_train = pd.read_csv ('C:/Users/z5023853/OneDrive - UNSW/Jupyter/Feature_selected/train_cat_ac.csv', \n",
    "                    names = [\"PT_cat\"], header = 1)\n",
    "L_train = pd.read_csv ('C:/Users/z5023853/OneDrive - UNSW/Jupyter/Feature_selected/train_cat_ac.csv', \n",
    "                    names = [\"Location\"], header = 1)\n",
    "yr_train = pd.read_csv ('C:/Users/z5023853/OneDrive - UNSW/Jupyter/Feature_selected/train_cat_ac.csv', \n",
    "                    names = [\"yr_cat\"], header = 1)\n",
    "contract_train = pd.read_csv ('C:/Users/z5023853/OneDrive - UNSW/Jupyter/Feature_selected/train_cat_ac.csv', \n",
    "                    names = [\"contract\"], header = 1)\n",
    "#CS_train = CS_train [2:,:]\n",
    "#BT_train = BT_train [2:,:]\n",
    "#per_train = per_train [2:,:]\n",
    "#PT_train = PT_train [2:,:]\n",
    "#L_train = L_train [2:,:]\n",
    "#yr_train = yr_train [2:,:]\n",
    "#contract_train = contract_train [2:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test set\n",
    "x_o_test = np.genfromtxt ('C:/Users/z5023853/OneDrive - UNSW/Jupyter/Feature_selected/test_cat_ac.csv', \n",
    "                    delimiter = ',', usecols = (11,12,13,7),skip_header =1)\n",
    "y_o_test = np.genfromtxt ('C:/Users/z5023853/OneDrive - UNSW/Jupyter/Feature_selected/test_cat_ac.csv', \n",
    "                    delimiter = ',', usecols = (46,47,50,51,52,54,55,57),skip_header =1)\n",
    "CS_test = pd.read_csv ('C:/Users/z5023853/OneDrive - UNSW/Jupyter/Feature_selected/test_cat_ac.csv', \n",
    "                    names = [\"CS_cat\"], header =0)\n",
    "BT_test = pd.read_csv ('C:/Users/z5023853/OneDrive - UNSW/Jupyter/Feature_selected/test_cat_ac.csv', \n",
    "                    names = [\"BT_cat\"], header =0)\n",
    "per_test = pd.read_csv ('C:/Users/z5023853/OneDrive - UNSW/Jupyter/Feature_selected/test_cat_ac.csv', \n",
    "                    names = [\"per_cat\"], header =0)\n",
    "PT_test = pd.read_csv ('C:/Users/z5023853/OneDrive - UNSW/Jupyter/Feature_selected/test_cat_ac.csv', \n",
    "                    names = [\"PT_cat\"], header = 0)\n",
    "L_test = pd.read_csv ('C:/Users/z5023853/OneDrive - UNSW/Jupyter/Feature_selected/test_cat_ac.csv', \n",
    "                    names = [\"Location\"], header =0)\n",
    "yr_test = pd.read_csv ('C:/Users/z5023853/OneDrive - UNSW/Jupyter/Feature_selected/test_cat_ac.csv', \n",
    "                    names = [\"yr_cat\"], header =0)\n",
    "contract_test = pd.read_csv ('C:/Users/z5023853/OneDrive - UNSW/Jupyter/Feature_selected/test_cat_ac.csv', \n",
    "                    names = [\"contract\"], header =0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = LabelBinarizer (neg_label=-1, pos_label=1)\n",
    "#CS\n",
    "lb.fit (CS_train)\n",
    "CS_train = lb.transform (CS_train)\n",
    "CS_test = lb.transform (CS_test)\n",
    "\n",
    "#BT\n",
    "lb.fit (BT_train)\n",
    "BT_train = lb.transform (BT_train)\n",
    "BT_test = lb.transform (BT_test)\n",
    "\n",
    "#per\n",
    "lb.fit (per_train)\n",
    "per_train = lb.transform (per_train)\n",
    "per_test = lb.transform (per_test)\n",
    "\n",
    "\n",
    "#PT\n",
    "lb.fit (PT_train)\n",
    "PT_train = lb.transform (PT_train)\n",
    "PT_test = lb.transform (PT_test)\n",
    "\n",
    "\n",
    "#Location\n",
    "lb.fit (L_train)\n",
    "L_train = lb.transform (L_train)\n",
    "L_test = lb.transform (L_test)\n",
    "\n",
    "\n",
    "#yr\n",
    "lb.fit (yr_train)\n",
    "yr_train = lb.transform (yr_train)\n",
    "yr_test = lb.transform (yr_test)\n",
    "\n",
    "\n",
    "#contract\n",
    "lb.fit (contract_train)\n",
    "contract_train = lb.transform (contract_train)\n",
    "contract_test = lb.transform (contract_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler ()\n",
    "scaler.fit (x_o_train)\n",
    "x_o_train = scaler.transform (x_o_train)\n",
    "x_o_test = scaler.transform (x_o_test)\n",
    "\n",
    "x_train_split = np.hstack ((x_o_train, CS_train, BT_train, per_train, PT_train, L_train, yr_train, contract_train))\n",
    "x_test = np.hstack ((x_o_test, CS_test, BT_test, per_test, PT_test, L_test, yr_test, contract_test))\n",
    "\n",
    "x_train, x_cv, y_train, y_cv = train_test_split (x_train_split,y_o_train,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(226, 11)\n",
      "(226, 8)\n",
      "(97, 11)\n"
     ]
    }
   ],
   "source": [
    "print (x_train.shape)\n",
    "print (y_train.shape)\n",
    "print (x_cv.shape)\n",
    "#print (CS_train.shape)\n",
    "#print (CS_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 10)                120       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 8)                 88        \n",
      "=================================================================\n",
      "Total params: 758\n",
      "Trainable params: 758\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 226 samples, validate on 97 samples\n",
      "Epoch 1/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.8766 - binary_accuracy: 0.6670 - val_loss: 2.7054 - val_binary_accuracy: 0.6843\n",
      "Epoch 2/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.7659 - binary_accuracy: 0.6764 - val_loss: 2.5786 - val_binary_accuracy: 0.6778\n",
      "Epoch 3/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.6142 - binary_accuracy: 0.6715 - val_loss: 2.4478 - val_binary_accuracy: 0.6611\n",
      "Epoch 4/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.4855 - binary_accuracy: 0.6698 - val_loss: 2.4220 - val_binary_accuracy: 0.6559\n",
      "Epoch 5/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.4055 - binary_accuracy: 0.6626 - val_loss: 2.3379 - val_binary_accuracy: 0.6302\n",
      "Epoch 6/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.3699 - binary_accuracy: 0.6477 - val_loss: 2.3037 - val_binary_accuracy: 0.6289\n",
      "Epoch 7/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.2920 - binary_accuracy: 0.6449 - val_loss: 2.2433 - val_binary_accuracy: 0.6314\n",
      "Epoch 8/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.2292 - binary_accuracy: 0.6405 - val_loss: 2.1759 - val_binary_accuracy: 0.6314\n",
      "Epoch 9/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.1849 - binary_accuracy: 0.6394 - val_loss: 2.1502 - val_binary_accuracy: 0.6314\n",
      "Epoch 10/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.1375 - binary_accuracy: 0.6410 - val_loss: 2.1154 - val_binary_accuracy: 0.6314\n",
      "<keras.callbacks.History object at 0x000000000E22D198>\n",
      "82/82 [==============================] - ETA: 0s\n",
      "[1.9231941554604508, 0.63109754498411974]\n",
      "[ 5.55424409] [-7.81876095] [-3.76212147] [ 5.09534374] [ 2.88647476] [ 6.38440818] [ 11.81466716] [-1.84750006] [-0.15214071]\n",
      "hamming_loss_train 0.3877212389380531\n",
      "hamming_loss_cv 0.4072164948453608\n",
      "hamming_loss_test 0.3826219512195122\n",
      "hamming_loss_traindataset 0.39357585139318885\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_22 (Dense)             (None, 11)                132       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 11)                0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 11)                132       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 11)                0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 11)                132       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 11)                0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 11)                132       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 11)                0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 11)                132       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 11)                0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 11)                132       \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 8)                 96        \n",
      "=================================================================\n",
      "Total params: 888\n",
      "Trainable params: 888\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 226 samples, validate on 97 samples\n",
      "Epoch 1/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 3.7161 - binary_accuracy: 0.6079 - val_loss: 3.4223 - val_binary_accuracy: 0.6198\n",
      "Epoch 2/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 3.4212 - binary_accuracy: 0.6101 - val_loss: 3.2770 - val_binary_accuracy: 0.6211\n",
      "Epoch 3/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 3.2343 - binary_accuracy: 0.6150 - val_loss: 3.0988 - val_binary_accuracy: 0.6302\n",
      "Epoch 4/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 3.0251 - binary_accuracy: 0.6206 - val_loss: 2.8961 - val_binary_accuracy: 0.6340\n",
      "Epoch 5/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.7790 - binary_accuracy: 0.6267 - val_loss: 2.6526 - val_binary_accuracy: 0.6405\n",
      "Epoch 6/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.6075 - binary_accuracy: 0.6311 - val_loss: 2.5647 - val_binary_accuracy: 0.6405\n",
      "Epoch 7/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.4416 - binary_accuracy: 0.6338 - val_loss: 2.5102 - val_binary_accuracy: 0.6379\n",
      "Epoch 8/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.2717 - binary_accuracy: 0.6399 - val_loss: 2.2480 - val_binary_accuracy: 0.6456\n",
      "Epoch 9/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.0630 - binary_accuracy: 0.6527 - val_loss: 2.0066 - val_binary_accuracy: 0.6598\n",
      "Epoch 10/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.8687 - binary_accuracy: 0.6593 - val_loss: 1.7239 - val_binary_accuracy: 0.6559\n",
      "<keras.callbacks.History object at 0x0000000012364E48>\n",
      "82/82 [==============================] - ETA: 0s\n",
      "[1.4284648400981252, 0.66615855403062774]\n",
      "[-0.14870879] [-0.37905574] [-0.70040693] [ 2.81846528] [-4.7896065] [-8.59473461] [-4.1893131] [-7.43414246] [ 0.8462098]\n",
      "hamming_loss_train 0.3915929203539823\n",
      "hamming_loss_cv 0.4007731958762887\n",
      "hamming_loss_test 0.3719512195121951\n",
      "hamming_loss_traindataset 0.3943498452012384\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_29 (Dense)             (None, 12)                144       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 8)                 104       \n",
      "=================================================================\n",
      "Total params: 1,028\n",
      "Trainable params: 1,028\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 226 samples, validate on 97 samples\n",
      "Epoch 1/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 3.4536 - binary_accuracy: 0.6405 - val_loss: 3.3606 - val_binary_accuracy: 0.6959\n",
      "Epoch 2/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 3.2150 - binary_accuracy: 0.6936 - val_loss: 2.9553 - val_binary_accuracy: 0.7010\n",
      "Epoch 3/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.8293 - binary_accuracy: 0.6952 - val_loss: 2.6641 - val_binary_accuracy: 0.6985\n",
      "Epoch 4/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.6702 - binary_accuracy: 0.6869 - val_loss: 2.5567 - val_binary_accuracy: 0.6778\n",
      "Epoch 5/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.4985 - binary_accuracy: 0.6692 - val_loss: 2.3805 - val_binary_accuracy: 0.6508\n",
      "Epoch 6/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.1857 - binary_accuracy: 0.6554 - val_loss: 1.8109 - val_binary_accuracy: 0.6340\n",
      "Epoch 7/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.6929 - binary_accuracy: 0.6327 - val_loss: 1.5866 - val_binary_accuracy: 0.6276\n",
      "Epoch 8/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.5221 - binary_accuracy: 0.6289 - val_loss: 1.5617 - val_binary_accuracy: 0.6302\n",
      "Epoch 9/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.4980 - binary_accuracy: 0.6327 - val_loss: 1.5469 - val_binary_accuracy: 0.6327\n",
      "Epoch 10/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.4821 - binary_accuracy: 0.6333 - val_loss: 1.5338 - val_binary_accuracy: 0.6405\n",
      "<keras.callbacks.History object at 0x00000000120AB7B8>\n",
      "82/82 [==============================] - ETA: 0s\n",
      "[1.3866125694135341, 0.6448170659018726]\n",
      "[ 2.03371971] [ 1.01182401] [-3.96232641] [ 1.04927195] [ 3.39512065] [-1.55987788] [ 1.77756938] [ 1.44149971] [-0.04338736]\n",
      "hamming_loss_train 0.5381637168141593\n",
      "hamming_loss_cv 0.5837628865979382\n",
      "hamming_loss_test 0.5533536585365854\n",
      "hamming_loss_traindataset 0.5518575851393189\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_36 (Dense)             (None, 13)                156       \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 13)                0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 13)                182       \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 13)                0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 13)                182       \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 13)                0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 13)                182       \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 13)                0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 13)                182       \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 13)                0         \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 13)                182       \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 8)                 112       \n",
      "=================================================================\n",
      "Total params: 1,178\n",
      "Trainable params: 1,178\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 226 samples, validate on 97 samples\n",
      "Epoch 1/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.7173 - binary_accuracy: 0.6040 - val_loss: 2.6990 - val_binary_accuracy: 0.6082\n",
      "Epoch 2/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.5599 - binary_accuracy: 0.6051 - val_loss: 2.6891 - val_binary_accuracy: 0.6070\n",
      "Epoch 3/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.5346 - binary_accuracy: 0.6067 - val_loss: 2.6736 - val_binary_accuracy: 0.6070\n",
      "Epoch 4/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.5138 - binary_accuracy: 0.6095 - val_loss: 2.6490 - val_binary_accuracy: 0.6070\n",
      "Epoch 5/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.5024 - binary_accuracy: 0.6123 - val_loss: 2.6253 - val_binary_accuracy: 0.6095\n",
      "Epoch 6/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.4880 - binary_accuracy: 0.6123 - val_loss: 2.6049 - val_binary_accuracy: 0.6095\n",
      "Epoch 7/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.4746 - binary_accuracy: 0.6123 - val_loss: 2.5801 - val_binary_accuracy: 0.6095\n",
      "Epoch 8/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.4668 - binary_accuracy: 0.6117 - val_loss: 2.5739 - val_binary_accuracy: 0.6121\n",
      "Epoch 9/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.4612 - binary_accuracy: 0.6134 - val_loss: 2.5565 - val_binary_accuracy: 0.6121\n",
      "Epoch 10/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.4405 - binary_accuracy: 0.6150 - val_loss: 2.5217 - val_binary_accuracy: 0.6121\n",
      "<keras.callbacks.History object at 0x0000000016CCBBA8>\n",
      "82/82 [==============================] - ETA: 0s\n",
      "[2.1307215167254938, 0.66310974446738635]\n",
      "[ 6.50334178] [ 7.93488542] [-4.1419291] [ 3.31124145] [ 7.75016884] [-0.26619256] [ 0.62389188] [-3.92530319] [-0.12380895]\n",
      "hamming_loss_train 0.39491150442477874\n",
      "hamming_loss_cv 0.38917525773195877\n",
      "hamming_loss_test 0.40853658536585363\n",
      "hamming_loss_traindataset 0.3931888544891641\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_43 (Dense)             (None, 14)                168       \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 14)                0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 14)                210       \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 14)                0         \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 14)                210       \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 14)                0         \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 14)                210       \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 14)                0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 14)                210       \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 14)                0         \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 14)                210       \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 8)                 120       \n",
      "=================================================================\n",
      "Total params: 1,338\n",
      "Trainable params: 1,338\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 226 samples, validate on 97 samples\n",
      "Epoch 1/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 3.0794 - binary_accuracy: 0.7035 - val_loss: 2.8366 - val_binary_accuracy: 0.7101\n",
      "Epoch 2/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.6884 - binary_accuracy: 0.7035 - val_loss: 2.3493 - val_binary_accuracy: 0.7101\n",
      "Epoch 3/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.2102 - binary_accuracy: 0.7035 - val_loss: 1.8079 - val_binary_accuracy: 0.7101\n",
      "Epoch 4/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.7913 - binary_accuracy: 0.7041 - val_loss: 1.3306 - val_binary_accuracy: 0.7152\n",
      "Epoch 5/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.4523 - binary_accuracy: 0.7030 - val_loss: 0.9818 - val_binary_accuracy: 0.7049\n",
      "Epoch 6/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.0885 - binary_accuracy: 0.6947 - val_loss: 0.7538 - val_binary_accuracy: 0.7036\n",
      "Epoch 7/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 0.8645 - binary_accuracy: 0.7035 - val_loss: 0.6393 - val_binary_accuracy: 0.7126\n",
      "Epoch 8/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 0.7498 - binary_accuracy: 0.7030 - val_loss: 0.6137 - val_binary_accuracy: 0.7126\n",
      "Epoch 9/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 0.7078 - binary_accuracy: 0.7008 - val_loss: 0.5941 - val_binary_accuracy: 0.7204\n",
      "Epoch 10/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 0.6426 - binary_accuracy: 0.6980 - val_loss: 0.5858 - val_binary_accuracy: 0.7204\n",
      "<keras.callbacks.History object at 0x000000001771AF98>\n",
      "82/82 [==============================] - ETA: 0s\n",
      "[0.56358864830761424, 0.71493901276006933]\n",
      "[-1.65962918] [ 0.42757668] [-1.54886121] [-1.11102554] [ 1.93254667] [ 0.29137101] [ 1.35172831] [ 1.66716711] [ 0.44099466]\n",
      "hamming_loss_train 0.45298672566371684\n",
      "hamming_loss_cv 0.49871134020618557\n",
      "hamming_loss_test 0.4375\n",
      "hamming_loss_traindataset 0.46671826625387\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_50 (Dense)             (None, 15)                180       \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 15)                240       \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 15)                240       \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 15)                240       \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 15)                240       \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 15)                240       \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 8)                 128       \n",
      "=================================================================\n",
      "Total params: 1,508\n",
      "Trainable params: 1,508\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 226 samples, validate on 97 samples\n",
      "Epoch 1/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 3.1190 - binary_accuracy: 0.6250 - val_loss: 3.0327 - val_binary_accuracy: 0.6211\n",
      "Epoch 2/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.8502 - binary_accuracy: 0.6233 - val_loss: 2.7486 - val_binary_accuracy: 0.6108\n",
      "Epoch 3/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.6378 - binary_accuracy: 0.6278 - val_loss: 2.4984 - val_binary_accuracy: 0.6186\n",
      "Epoch 4/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.4748 - binary_accuracy: 0.6366 - val_loss: 2.2617 - val_binary_accuracy: 0.6237\n",
      "Epoch 5/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.2543 - binary_accuracy: 0.6388 - val_loss: 2.1378 - val_binary_accuracy: 0.6289\n",
      "Epoch 6/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.1432 - binary_accuracy: 0.6460 - val_loss: 2.0907 - val_binary_accuracy: 0.6418\n",
      "Epoch 7/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.9854 - binary_accuracy: 0.6488 - val_loss: 1.9238 - val_binary_accuracy: 0.6495\n",
      "Epoch 8/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.8022 - binary_accuracy: 0.6621 - val_loss: 1.6933 - val_binary_accuracy: 0.6469\n",
      "Epoch 9/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.6444 - binary_accuracy: 0.6643 - val_loss: 1.5616 - val_binary_accuracy: 0.6637\n",
      "Epoch 10/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.4149 - binary_accuracy: 0.6715 - val_loss: 1.3642 - val_binary_accuracy: 0.6649\n",
      "<keras.callbacks.History object at 0x0000000018F8FEB8>\n",
      "82/82 [==============================] - ETA: 0s\n",
      "[1.1426718438543924, 0.68292683944469545]\n",
      "[ 5.95084002] [-3.70138281] [-1.16379632] [ 0.97439601] [ 4.20736016] [-2.93143896] [-2.9300966] [-1.95864861] [-0.28186748]\n",
      "hamming_loss_train 0.4043141592920354\n",
      "hamming_loss_cv 0.4329896907216495\n",
      "hamming_loss_test 0.4329268292682927\n",
      "hamming_loss_traindataset 0.41292569659442724\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_57 (Dense)             (None, 16)                192       \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (None, 8)                 136       \n",
      "=================================================================\n",
      "Total params: 1,688\n",
      "Trainable params: 1,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 226 samples, validate on 97 samples\n",
      "Epoch 1/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 3.0800 - binary_accuracy: 0.6997 - val_loss: 2.3910 - val_binary_accuracy: 0.7062\n",
      "Epoch 2/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.5572 - binary_accuracy: 0.6969 - val_loss: 2.2316 - val_binary_accuracy: 0.6985\n",
      "Epoch 3/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.3418 - binary_accuracy: 0.6914 - val_loss: 2.0352 - val_binary_accuracy: 0.6920\n",
      "Epoch 4/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.0939 - binary_accuracy: 0.6764 - val_loss: 1.8495 - val_binary_accuracy: 0.6714\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226/226 [==============================] - ETA: 0s - loss: 1.8840 - binary_accuracy: 0.6643 - val_loss: 1.6678 - val_binary_accuracy: 0.6559\n",
      "Epoch 6/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.7156 - binary_accuracy: 0.6593 - val_loss: 1.6142 - val_binary_accuracy: 0.6508\n",
      "Epoch 7/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.5893 - binary_accuracy: 0.6521 - val_loss: 1.5015 - val_binary_accuracy: 0.6456\n",
      "Epoch 8/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.5062 - binary_accuracy: 0.6482 - val_loss: 1.4740 - val_binary_accuracy: 0.6482\n",
      "Epoch 9/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.4423 - binary_accuracy: 0.6460 - val_loss: 1.4387 - val_binary_accuracy: 0.6495\n",
      "Epoch 10/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.3906 - binary_accuracy: 0.6388 - val_loss: 1.4179 - val_binary_accuracy: 0.6430\n",
      "<keras.callbacks.History object at 0x0000000019BCC780>\n",
      "82/82 [==============================] - ETA: 0s\n",
      "[1.1731089615240329, 0.67073171894724781]\n",
      "[ 1.16704079] [-1.05382788] [ 0.03280007] [-5.90706639] [ 1.52116965] [-1.07878117] [-1.4893756] [-2.98675699] [ 0.09791884]\n",
      "hamming_loss_train 0.39214601769911506\n",
      "hamming_loss_cv 0.40850515463917525\n",
      "hamming_loss_test 0.39634146341463417\n",
      "hamming_loss_traindataset 0.39705882352941174\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_64 (Dense)             (None, 17)                204       \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 17)                0         \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 17)                306       \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 17)                0         \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 17)                306       \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 17)                0         \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 17)                306       \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 17)                0         \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 17)                306       \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 17)                0         \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 17)                306       \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 8)                 144       \n",
      "=================================================================\n",
      "Total params: 1,878\n",
      "Trainable params: 1,878\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 226 samples, validate on 97 samples\n",
      "Epoch 1/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.8075 - binary_accuracy: 0.6914 - val_loss: 2.5420 - val_binary_accuracy: 0.7049\n",
      "Epoch 2/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.4990 - binary_accuracy: 0.6975 - val_loss: 2.3784 - val_binary_accuracy: 0.7036\n",
      "Epoch 3/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.2348 - binary_accuracy: 0.6997 - val_loss: 1.9303 - val_binary_accuracy: 0.7023\n",
      "Epoch 4/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.6376 - binary_accuracy: 0.6991 - val_loss: 1.4594 - val_binary_accuracy: 0.6894\n",
      "Epoch 5/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.3719 - binary_accuracy: 0.6969 - val_loss: 1.3018 - val_binary_accuracy: 0.6817\n",
      "Epoch 6/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.2966 - binary_accuracy: 0.6842 - val_loss: 1.2606 - val_binary_accuracy: 0.6791\n",
      "Epoch 7/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.2329 - binary_accuracy: 0.6820 - val_loss: 1.2028 - val_binary_accuracy: 0.6817\n",
      "Epoch 8/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.1596 - binary_accuracy: 0.6925 - val_loss: 1.0618 - val_binary_accuracy: 0.6843\n",
      "Epoch 9/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.0973 - binary_accuracy: 0.7046 - val_loss: 1.0050 - val_binary_accuracy: 0.6869\n",
      "Epoch 10/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.0264 - binary_accuracy: 0.7124 - val_loss: 0.9042 - val_binary_accuracy: 0.6894\n",
      "<keras.callbacks.History object at 0x000000001B435780>\n",
      "82/82 [==============================] - ETA: 0s\n",
      "[0.77383370661154027, 0.7301829340981274]\n",
      "[-3.26171374] [ 2.04030081] [ 1.2285157] [ 0.09645782] [-1.42135659] [-0.62480848] [-2.71150764] [ 1.64716258] [ 0.94879947]\n",
      "hamming_loss_train 0.4823008849557522\n",
      "hamming_loss_cv 0.4806701030927835\n",
      "hamming_loss_test 0.4405487804878049\n",
      "hamming_loss_traindataset 0.4818111455108359\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_71 (Dense)             (None, 18)                216       \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 18)                0         \n",
      "_________________________________________________________________\n",
      "dense_72 (Dense)             (None, 18)                342       \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 18)                0         \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 18)                342       \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 18)                0         \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 18)                342       \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 18)                0         \n",
      "_________________________________________________________________\n",
      "dense_75 (Dense)             (None, 18)                342       \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 18)                0         \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 18)                342       \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 8)                 152       \n",
      "=================================================================\n",
      "Total params: 2,078\n",
      "Trainable params: 2,078\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 226 samples, validate on 97 samples\n",
      "Epoch 1/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.0980 - binary_accuracy: 0.6305 - val_loss: 1.8250 - val_binary_accuracy: 0.6714\n",
      "Epoch 2/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.9933 - binary_accuracy: 0.6521 - val_loss: 1.7915 - val_binary_accuracy: 0.6791\n",
      "Epoch 3/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.9516 - binary_accuracy: 0.6549 - val_loss: 1.7830 - val_binary_accuracy: 0.6894\n",
      "Epoch 4/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.9406 - binary_accuracy: 0.6549 - val_loss: 1.7802 - val_binary_accuracy: 0.6997\n",
      "Epoch 5/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.8874 - binary_accuracy: 0.6621 - val_loss: 1.7782 - val_binary_accuracy: 0.7062\n",
      "Epoch 6/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.8686 - binary_accuracy: 0.6659 - val_loss: 1.7534 - val_binary_accuracy: 0.7126\n",
      "Epoch 7/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.8610 - binary_accuracy: 0.6781 - val_loss: 1.7339 - val_binary_accuracy: 0.7062\n",
      "Epoch 8/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.8648 - binary_accuracy: 0.6875 - val_loss: 1.7454 - val_binary_accuracy: 0.7126\n",
      "Epoch 9/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.8677 - binary_accuracy: 0.6925 - val_loss: 1.7434 - val_binary_accuracy: 0.7126\n",
      "Epoch 10/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.8542 - binary_accuracy: 0.6947 - val_loss: 1.7409 - val_binary_accuracy: 0.7191\n",
      "<keras.callbacks.History object at 0x000000001BF66CC0>\n",
      "82/82 [==============================] - ETA: 0s\n",
      "[1.6556191473472408, 0.70121951219512191]\n",
      "[ 0.29410637] [ 4.1823829] [ 2.50074849] [ 0.39934441] [ 6.42058063] [-0.91986694] [ 0.88261898] [-0.759057] [ 0.12140522]\n",
      "hamming_loss_train 0.3329646017699115\n",
      "hamming_loss_cv 0.327319587628866\n",
      "hamming_loss_test 0.3475609756097561\n",
      "hamming_loss_traindataset 0.33126934984520123\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_78 (Dense)             (None, 19)                228       \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 19)                0         \n",
      "_________________________________________________________________\n",
      "dense_79 (Dense)             (None, 19)                380       \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 19)                0         \n",
      "_________________________________________________________________\n",
      "dense_80 (Dense)             (None, 19)                380       \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 19)                0         \n",
      "_________________________________________________________________\n",
      "dense_81 (Dense)             (None, 19)                380       \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 19)                0         \n",
      "_________________________________________________________________\n",
      "dense_82 (Dense)             (None, 19)                380       \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 19)                0         \n",
      "_________________________________________________________________\n",
      "dense_83 (Dense)             (None, 19)                380       \n",
      "_________________________________________________________________\n",
      "dense_84 (Dense)             (None, 8)                 160       \n",
      "=================================================================\n",
      "Total params: 2,288\n",
      "Trainable params: 2,288\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 226 samples, validate on 97 samples\n",
      "Epoch 1/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 3.5841 - binary_accuracy: 0.5857 - val_loss: 3.0193 - val_binary_accuracy: 0.6173\n",
      "Epoch 2/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.9217 - binary_accuracy: 0.6112 - val_loss: 2.6281 - val_binary_accuracy: 0.6366\n",
      "Epoch 3/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.6241 - binary_accuracy: 0.6322 - val_loss: 2.3073 - val_binary_accuracy: 0.6688\n",
      "Epoch 4/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.4774 - binary_accuracy: 0.6421 - val_loss: 2.1382 - val_binary_accuracy: 0.6611\n",
      "Epoch 5/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.3168 - binary_accuracy: 0.6388 - val_loss: 1.9296 - val_binary_accuracy: 0.6611\n",
      "Epoch 6/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.0460 - binary_accuracy: 0.6366 - val_loss: 1.8905 - val_binary_accuracy: 0.6585\n",
      "Epoch 7/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.0146 - binary_accuracy: 0.6289 - val_loss: 1.8733 - val_binary_accuracy: 0.6521\n",
      "Epoch 8/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.9933 - binary_accuracy: 0.6250 - val_loss: 1.7921 - val_binary_accuracy: 0.6469\n",
      "Epoch 9/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.8740 - binary_accuracy: 0.6294 - val_loss: 1.7108 - val_binary_accuracy: 0.6418\n",
      "Epoch 10/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.7340 - binary_accuracy: 0.6200 - val_loss: 1.7138 - val_binary_accuracy: 0.6405\n",
      "<keras.callbacks.History object at 0x000000001C9F7CF8>\n",
      "82/82 [==============================] - ETA: 0s\n",
      "[1.5323174290540742, 0.6402438908088498]\n",
      "[ 0.11011401] [ 1.02254207] [ 0.20646874] [-0.58663818] [ 0.28081074] [-0.14578567] [-0.49566486] [ 0.72802263] [ 0.09972702]\n",
      "hamming_loss_train 0.4618362831858407\n",
      "hamming_loss_cv 0.48711340206185566\n",
      "hamming_loss_test 0.46189024390243905\n",
      "hamming_loss_traindataset 0.4694272445820433\n"
     ]
    }
   ],
   "source": [
    "y_test = y_o_test;\n",
    "Neuron = 13;\n",
    "num_classes = 8;\n",
    "for Neuron in range (10,20):\n",
    "    model = Sequential ()\n",
    "    model.add (Dense (units = Neuron, input_dim = 11))\n",
    "    model.add (layers.Activation ('tanh'))\n",
    "    model.add (Dense (units = Neuron))\n",
    "    model.add (layers.Activation ('tanh'))\n",
    "    model.add (Dense (units = Neuron))\n",
    "    model.add (layers.Activation ('tanh'))\n",
    "    model.add (Dense (units = Neuron))\n",
    "    model.add (layers.Activation ('tanh'))\n",
    "    model.add (Dense (units = Neuron))\n",
    "    model.add (layers.Activation ('tanh'))\n",
    "    model.add (Dense (units = Neuron))\n",
    "    model.add (Dense (num_classes))\n",
    "    print (model.summary())\n",
    "\n",
    "#sgd = optimizer.SGD (lr=0.01, momentum = 0, decay =0, nestrov = False)\n",
    "    model.compile (loss = 'binary_crossentropy', optimizer = 'adam', metrics=['binary_accuracy'])\n",
    "    print(model.fit(x_train, y_train, batch_size=80, verbose=1, validation_data=(x_cv, y_cv)))\n",
    "    print(model.evaluate (x_test, y_test, batch_size = 50, verbose =1))\n",
    "#help (model.evaluate)\n",
    "#this is the output units\n",
    "    y_pred_train = model.predict (x_train)\n",
    "    y_pred_cv = model.predict (x_cv)\n",
    "    y_pred_test = model.predict (x_test)\n",
    "    y_pred_whole = np.vstack ((y_pred_train, y_pred_cv)).astype(np.float32)\n",
    "    \n",
    "    print('hamming_loss_train',hamming_loss(y_train, y_train_label))\n",
    "    print('hamming_loss_cv',hamming_loss(y_cv, y_cv_label))\n",
    "    print('hamming_loss_test',hamming_loss(y_test, y_test_label))\n",
    "    print('hamming_loss_traindataset',hamming_loss(y_t, y_t_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "#save model to json, weights to HDF5\n",
    "model_json = model.to_json ()\n",
    "with open ('trainC.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights('trainC.h5')\n",
    "print ('Saved model to disk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load json and create model\n",
    "json_file = open('trainC.json', 'r')\n",
    "loaded_model_json = json_file.read ()\n",
    "json_file.close ()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "loaded_model.load_weights('trainC.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = loaded_model.predict (x_train)\n",
    "y_pred_cv = loaded_model.predict (x_cv)\n",
    "y_pred_test = loaded_model.predict (x_test)\n",
    "y_pred_whole = np.vstack ((y_pred_train, y_pred_cv)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'k': 8, 's': 0.4} 0.395343627747\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "parameters = {'k': range (8,9), 's': [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]}\n",
    "score = 'f1_micro'\n",
    "knn = GridSearchCV (MLkNN(), parameters, scoring =score)\n",
    "knn.fit (y_pred_train,y_train)\n",
    "print (knn.best_params_, knn.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 4)\t1\n",
      "  (2, 1)\t1\n",
      "  (6, 1)\t1\n",
      "  (6, 4)\t1\n",
      "  (6, 6)\t1\n",
      "  (7, 4)\t1\n",
      "  (7, 6)\t1\n",
      "  (8, 4)\t1\n",
      "  (9, 4)\t1\n",
      "  (12, 1)\t1\n",
      "  (12, 6)\t1\n",
      "  (16, 1)\t1\n",
      "  (20, 1)\t1\n",
      "  (23, 1)\t1\n",
      "  (24, 1)\t1\n",
      "  (24, 6)\t1\n",
      "  (25, 1)\t1\n",
      "  (26, 1)\t1\n",
      "  (27, 4)\t1\n",
      "  (28, 1)\t1\n",
      "  (28, 2)\t1\n",
      "  (29, 6)\t1\n",
      "  (30, 1)\t1\n",
      "  (30, 6)\t1\n",
      "  (31, 6)\t1\n",
      "  (32, 1)\t1\n",
      "  (33, 1)\t1\n",
      "  (34, 4)\t1\n",
      "  (34, 6)\t1\n",
      "  (38, 6)\t1\n",
      "  (39, 1)\t1\n",
      "  (39, 2)\t1\n",
      "  (39, 6)\t1\n",
      "  (40, 1)\t1\n",
      "  (40, 2)\t1\n",
      "  (41, 2)\t1\n",
      "  (42, 1)\t1\n",
      "  (43, 4)\t1\n",
      "  (45, 1)\t1\n",
      "  (45, 4)\t1\n",
      "  (45, 5)\t1\n",
      "  (45, 6)\t1\n",
      "  (46, 1)\t1\n",
      "  (48, 6)\t1\n",
      "  (49, 1)\t1\n",
      "  (51, 1)\t1\n",
      "  (51, 2)\t1\n",
      "  (52, 1)\t1\n",
      "  (53, 1)\t1\n",
      "  (54, 1)\t1\n",
      "  (54, 4)\t1\n",
      "  (56, 1)\t1\n",
      "  (58, 1)\t1\n",
      "  (60, 1)\t1\n",
      "  (60, 2)\t1\n",
      "  (61, 1)\t1\n",
      "  (61, 4)\t1\n",
      "  (61, 6)\t1\n",
      "  (62, 1)\t1\n",
      "  (62, 6)\t1\n",
      "  (63, 4)\t1\n",
      "  (63, 6)\t1\n",
      "  (66, 1)\t1\n",
      "  (68, 1)\t1\n",
      "  (71, 4)\t1\n",
      "  (71, 6)\t1\n",
      "  (72, 1)\t1\n",
      "  (74, 1)\t1\n",
      "  (75, 4)\t1\n",
      "  (76, 1)\t1\n",
      "  (76, 4)\t1\n",
      "  (77, 1)\t1\n",
      "  (77, 5)\t1\n",
      "  (78, 1)\t1\n",
      "  (78, 2)\t1\n",
      "  (80, 1)\t1\n",
      "  (83, 4)\t1\n",
      "  (83, 6)\t1\n",
      "  (84, 1)\t1\n",
      "  (85, 4)\t1\n",
      "  (87, 1)\t1\n",
      "  (87, 4)\t1\n",
      "  (87, 6)\t1\n",
      "  (88, 1)\t1\n",
      "  (90, 1)\t1\n",
      "  (90, 4)\t1\n",
      "  (90, 6)\t1\n",
      "  (92, 1)\t1\n",
      "  (92, 6)\t1\n",
      "  (93, 4)\t1\n",
      "  (93, 6)\t1\n",
      "  (94, 1)\t1\n",
      "  (94, 2)\t1\n",
      "  (95, 1)\t1\n",
      "  (96, 1)\t1\n",
      "  (96, 4)\t1\n",
      "  (97, 1)\t1\n",
      "  (97, 6)\t1\n",
      "  (99, 2)\t1\n",
      "  (102, 1)\t1\n",
      "  (102, 4)\t1\n",
      "  (102, 5)\t1\n",
      "  (102, 6)\t1\n",
      "  (103, 4)\t1\n",
      "  (103, 6)\t1\n",
      "  (104, 6)\t1\n",
      "  (105, 1)\t1\n",
      "  (105, 4)\t1\n",
      "  (106, 4)\t1\n",
      "  (107, 6)\t1\n",
      "  (108, 1)\t1\n",
      "  (108, 6)\t1\n",
      "  (109, 1)\t1\n",
      "  (109, 2)\t1\n",
      "  (111, 2)\t1\n",
      "  (113, 1)\t1\n",
      "  (113, 2)\t1\n",
      "  (114, 1)\t1\n",
      "  (114, 4)\t1\n",
      "  (115, 1)\t1\n",
      "  (116, 1)\t1\n",
      "  (116, 2)\t1\n",
      "  (117, 2)\t1\n",
      "  (119, 1)\t1\n",
      "  (120, 4)\t1\n",
      "  (121, 6)\t1\n",
      "  (122, 4)\t1\n",
      "  (124, 1)\t1\n",
      "  (124, 4)\t1\n",
      "  (125, 1)\t1\n",
      "  (126, 1)\t1\n",
      "  (126, 4)\t1\n",
      "  (126, 6)\t1\n",
      "  (128, 4)\t1\n",
      "  (128, 6)\t1\n",
      "  (129, 1)\t1\n",
      "  (129, 5)\t1\n",
      "  (130, 1)\t1\n",
      "  (130, 6)\t1\n",
      "  (131, 6)\t1\n",
      "  (135, 1)\t1\n",
      "  (137, 1)\t1\n",
      "  (138, 4)\t1\n",
      "  (139, 2)\t1\n",
      "  (140, 4)\t1\n",
      "  (141, 1)\t1\n",
      "  (142, 1)\t1\n",
      "  (143, 1)\t1\n",
      "  (143, 2)\t1\n",
      "  (145, 4)\t1\n",
      "  (145, 6)\t1\n",
      "  (146, 4)\t1\n",
      "  (147, 1)\t1\n",
      "  (148, 4)\t1\n",
      "  (148, 6)\t1\n",
      "  (149, 1)\t1\n",
      "  (149, 4)\t1\n",
      "  (150, 1)\t1\n",
      "  (150, 6)\t1\n",
      "  (153, 1)\t1\n",
      "  (153, 6)\t1\n",
      "  (154, 6)\t1\n",
      "  (159, 1)\t1\n",
      "  (159, 6)\t1\n",
      "  (161, 1)\t1\n",
      "  (161, 6)\t1\n",
      "  (162, 1)\t1\n",
      "  (162, 6)\t1\n",
      "  (163, 1)\t1\n",
      "  (163, 4)\t1\n",
      "  (164, 1)\t1\n",
      "  (165, 1)\t1\n",
      "  (166, 1)\t1\n",
      "  (166, 4)\t1\n",
      "  (166, 6)\t1\n",
      "  (169, 1)\t1\n",
      "  (170, 4)\t1\n",
      "  (170, 6)\t1\n",
      "  (175, 1)\t1\n",
      "  (175, 4)\t1\n",
      "  (177, 2)\t1\n",
      "  (178, 1)\t1\n",
      "  (178, 2)\t1\n",
      "  (179, 1)\t1\n",
      "  (179, 4)\t1\n",
      "  (179, 6)\t1\n",
      "  (180, 6)\t1\n",
      "  (181, 1)\t1\n",
      "  (181, 6)\t1\n",
      "  (182, 4)\t1\n",
      "  (183, 1)\t1\n",
      "  (183, 4)\t1\n",
      "  (183, 6)\t1\n",
      "  (184, 4)\t1\n",
      "  (185, 4)\t1\n",
      "  (186, 1)\t1\n",
      "  (186, 6)\t1\n",
      "  (190, 1)\t1\n",
      "  (192, 1)\t1\n",
      "  (192, 4)\t1\n",
      "  (192, 5)\t1\n",
      "  (192, 6)\t1\n",
      "  (196, 1)\t1\n",
      "  (197, 4)\t1\n",
      "  (198, 1)\t1\n",
      "  (200, 1)\t1\n",
      "  (200, 2)\t1\n",
      "  (201, 1)\t1\n",
      "  (203, 1)\t1\n",
      "  (203, 4)\t1\n",
      "  (205, 5)\t1\n",
      "  (207, 6)\t1\n",
      "  (210, 1)\t1\n",
      "  (212, 1)\t1\n",
      "  (212, 4)\t1\n",
      "  (212, 6)\t1\n",
      "  (213, 1)\t1\n",
      "  (213, 4)\t1\n",
      "  (214, 6)\t1\n",
      "  (217, 1)\t1\n",
      "  (217, 6)\t1\n",
      "  (218, 1)\t1\n",
      "  (219, 1)\t1\n",
      "  (222, 1)\t1\n",
      "  (222, 2)\t1\n",
      "  (223, 1)\t1\n",
      "  (223, 6)\t1\n",
      "  (224, 1)\t1\n",
      "  (224, 2)\t1\n",
      "  (225, 4)\t1\n"
     ]
    }
   ],
   "source": [
    "print (knn.predict(y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t0.000755857898715\n",
      "  (0, 1)\t0.0961228787316\n",
      "  (0, 2)\t0.0903623452124\n",
      "  (0, 3)\t0.037249123747\n",
      "  (0, 4)\t0.0145236204366\n",
      "  (0, 5)\t0.0588320955293\n",
      "  (0, 6)\t0.0522302030682\n",
      "  (0, 7)\t0.000195963158926\n",
      "  (1, 0)\t0.000755857898715\n",
      "  (1, 1)\t0.117578878448\n",
      "  (1, 2)\t0.0439144107574\n",
      "  (1, 3)\t0.0412117964861\n",
      "  (1, 4)\t0.0914133756889\n",
      "  (1, 5)\t0.0302331602026\n",
      "  (1, 6)\t0.0522302030682\n",
      "  (1, 7)\t0.000195963158926\n",
      "  (2, 0)\t0.000755857898715\n",
      "  (2, 1)\t0.0703756790713\n",
      "  (2, 2)\t0.0439144107574\n",
      "  (2, 3)\t0.033286451008\n",
      "  (2, 4)\t0.0316102327148\n",
      "  (2, 5)\t0.0588320955293\n",
      "  (2, 6)\t0.0185332978629\n",
      "  (2, 7)\t0.000195963158926\n",
      "  (3, 0)\t0.000755857898715\n",
      "  (3, 1)\t0.0188812797508\n",
      "  (3, 2)\t0.0903623452124\n",
      "  (3, 3)\t0.037249123747\n",
      "  (3, 4)\t0.099956681828\n",
      "  (3, 5)\t0.0506609711503\n",
      "  (3, 6)\t0.0606544293695\n",
      "  (3, 7)\t0.000195963158926\n",
      "  (4, 0)\t0.000755857898715\n",
      "  (4, 1)\t0.0188812797508\n",
      "  (4, 2)\t0.0903623452124\n",
      "  (4, 3)\t0.00158506909562\n",
      "  (4, 4)\t0.0316102327148\n",
      "  (4, 5)\t0.0588320955293\n",
      "  (4, 6)\t0.0606544293695\n",
      "  (4, 7)\t0.000195963158926\n",
      "  (5, 0)\t0.000755857898715\n",
      "  (5, 1)\t0.0832492789015\n",
      "  (5, 2)\t0.0903623452124\n",
      "  (5, 3)\t0.00158506909562\n",
      "  (5, 4)\t0.129858253315\n",
      "  (5, 5)\t0.0302331602026\n",
      "  (5, 6)\t0.0522302030682\n",
      "  (5, 7)\t0.000195963158926\n",
      "  (6, 0)\t0.00453514739229\n",
      "  (6, 1)\t0.117578878448\n",
      "  (6, 2)\t0.0439144107574\n",
      "  (6, 3)\t0.033286451008\n",
      "  (6, 4)\t0.0914133756889\n",
      "  (6, 5)\t0.0588320955293\n",
      "  (6, 6)\t0.0395938636162\n",
      "  (6, 7)\t0.000195963158926\n",
      "  (7, 0)\t0.00453514739229\n",
      "  (7, 1)\t0.0832492789015\n",
      "  (7, 2)\t0.0312467922697\n",
      "  (7, 3)\t0.00158506909562\n",
      "  (7, 4)\t0.0914133756889\n",
      "  (7, 5)\t0.0588320955293\n",
      "  (7, 6)\t0.0353817504655\n",
      "  (7, 7)\t0.000195963158926\n",
      "  (8, 0)\t0.000755857898715\n",
      "  (8, 1)\t0.0961228787316\n",
      "  (8, 2)\t0.0819172662206\n",
      "  (8, 3)\t0.037249123747\n",
      "  (8, 4)\t0.0145236204366\n",
      "  (8, 5)\t0.0302331602026\n",
      "  (8, 6)\t0.0522302030682\n",
      "  (8, 7)\t0.000195963158926\n",
      "  (9, 0)\t0.000755857898715\n",
      "  (9, 1)\t0.0961228787316\n",
      "  (9, 2)\t0.0439144107574\n",
      "  (9, 3)\t0.033286451008\n",
      "  (9, 4)\t0.0657834572714\n",
      "  (9, 5)\t0.0506609711503\n",
      "  (9, 6)\t0.0185332978629\n",
      "  (9, 7)\t0.000195963158926\n",
      "  (10, 0)\t0.000755857898715\n",
      "  (10, 1)\t0.0188812797508\n",
      "  (10, 2)\t0.0903623452124\n",
      "  (10, 3)\t0.00158506909562\n",
      "  (10, 4)\t0.0316102327148\n",
      "  (10, 5)\t0.0588320955293\n",
      "  (10, 6)\t0.0606544293695\n",
      "  (10, 7)\t0.000195963158926\n",
      "  (11, 0)\t0.00453514739229\n",
      "  (11, 1)\t0.0188812797508\n",
      "  (11, 2)\t0.0312467922697\n",
      "  (11, 3)\t0.037249123747\n",
      "  (11, 4)\t0.129858253315\n",
      "  (11, 5)\t0.0588320955293\n",
      "  (11, 6)\t0.0185332978629\n",
      "  (11, 7)\t0.000195963158926\n",
      "  (12, 0)\t0.000755857898715\n",
      "  (12, 1)\t0.104705278618\n",
      "  (12, 2)\t0.0312467922697\n",
      "  (12, 3)\t0.033286451008\n",
      "  (12, 4)\t0.129858253315\n",
      "  (12, 5)\t0.00163422487582\n",
      "  (12, 6)\t0.0648665425202\n",
      "  (12, 7)\t0.000195963158926\n",
      "  (13, 0)\t0.000755857898715\n",
      "  (13, 1)\t0.0961228787316\n",
      "  (13, 2)\t0.0312467922697\n",
      "  (13, 3)\t0.00158506909562\n",
      "  (13, 4)\t0.099956681828\n",
      "  (13, 5)\t0.00163422487582\n",
      "  (13, 6)\t0.0606544293695\n",
      "  (13, 7)\t0.000195963158926\n",
      "  (14, 0)\t0.000755857898715\n",
      "  (14, 1)\t0.00171647997735\n",
      "  (14, 2)\t0.00168901579836\n",
      "  (14, 3)\t0.00158506909562\n",
      "  (14, 4)\t0.00170866122783\n",
      "  (14, 5)\t0.00163422487582\n",
      "  (14, 6)\t0.00168484526026\n",
      "  (14, 7)\t0.000195963158926\n",
      "  (15, 0)\t0.000755857898715\n",
      "  (15, 1)\t0.00171647997735\n",
      "  (15, 2)\t0.00168901579836\n",
      "  (15, 3)\t0.00158506909562\n",
      "  (15, 4)\t0.00170866122783\n",
      "  (15, 5)\t0.00163422487582\n",
      "  (15, 6)\t0.00168484526026\n",
      "  (15, 7)\t0.000195963158926\n",
      "  (16, 0)\t0.000755857898715\n",
      "  (16, 1)\t0.0703756790713\n",
      "  (16, 2)\t0.0903623452124\n",
      "  (16, 3)\t0.033286451008\n",
      "  (16, 4)\t0.129858253315\n",
      "  (16, 5)\t0.0588320955293\n",
      "  (16, 6)\t0.00168484526026\n",
      "  (16, 7)\t0.000195963158926\n",
      "  (17, 0)\t0.000755857898715\n",
      "  (17, 1)\t0.0188812797508\n",
      "  (17, 2)\t0.0903623452124\n",
      "  (17, 3)\t0.00158506909562\n",
      "  (17, 4)\t0.0316102327148\n",
      "  (17, 5)\t0.0588320955293\n",
      "  (17, 6)\t0.0606544293695\n",
      "  (17, 7)\t0.000195963158926\n",
      "  (18, 0)\t0.000755857898715\n",
      "  (18, 1)\t0.0832492789015\n",
      "  (18, 2)\t0.0819172662206\n",
      "  (18, 3)\t0.0412117964861\n",
      "  (18, 4)\t0.099956681828\n",
      "  (18, 5)\t0.0302331602026\n",
      "  (18, 6)\t0.0606544293695\n",
      "  (18, 7)\t0.000195963158926\n",
      "  (19, 0)\t0.000755857898715\n",
      "  (19, 1)\t0.00171647997735\n",
      "  (19, 2)\t0.00168901579836\n",
      "  (19, 3)\t0.00158506909562\n",
      "  (19, 4)\t0.00170866122783\n",
      "  (19, 5)\t0.00163422487582\n",
      "  (19, 6)\t0.00168484526026\n",
      "  (19, 7)\t0.000195963158926\n",
      "  (20, 0)\t0.000755857898715\n",
      "  (20, 1)\t0.104705278618\n",
      "  (20, 2)\t0.0903623452124\n",
      "  (20, 3)\t0.0412117964861\n",
      "  (20, 4)\t0.099956681828\n",
      "  (20, 5)\t0.0588320955293\n",
      "  (20, 6)\t0.00168484526026\n",
      "  (20, 7)\t0.000195963158926\n",
      "  (21, 0)\t0.000755857898715\n",
      "  (21, 1)\t0.0832492789015\n",
      "  (21, 2)\t0.0903623452124\n",
      "  (21, 3)\t0.037249123747\n",
      "  (21, 4)\t0.129858253315\n",
      "  (21, 5)\t0.0506609711503\n",
      "  (21, 6)\t0.0185332978629\n",
      "  (21, 7)\t0.000195963158926\n",
      "  (22, 0)\t0.000755857898715\n",
      "  (22, 1)\t0.0961228787316\n",
      "  (22, 2)\t0.0312467922697\n",
      "  (22, 3)\t0.037249123747\n",
      "  (22, 4)\t0.129858253315\n",
      "  (22, 5)\t0.0302331602026\n",
      "  (22, 6)\t0.00168484526026\n",
      "  (22, 7)\t0.000195963158926\n",
      "  (23, 0)\t0.000755857898715\n",
      "  (23, 1)\t0.117578878448\n",
      "  (23, 2)\t0.0312467922697\n",
      "  (23, 3)\t0.033286451008\n",
      "  (23, 4)\t0.099956681828\n",
      "  (23, 5)\t0.0506609711503\n",
      "  (23, 6)\t0.00168484526026\n",
      "  (23, 7)\t0.000195963158926\n",
      "  (24, 0)\t0.000755857898715\n",
      "  (24, 1)\t0.104705278618\n",
      "  (24, 2)\t0.0819172662206\n",
      "  (24, 3)\t0.0412117964861\n",
      "  (24, 4)\t0.129858253315\n",
      "  (24, 5)\t0.0302331602026\n",
      "  (24, 6)\t0.0648665425202\n",
      "  (24, 7)\t0.000195963158926\n",
      "  (25, 0)\t0.000755857898715\n",
      "  (25, 1)\t0.0703756790713\n",
      "  (25, 2)\t0.0903623452124\n",
      "  (25, 3)\t0.033286451008\n",
      "  (25, 4)\t0.129858253315\n",
      "  (25, 5)\t0.0588320955293\n",
      "  (25, 6)\t0.00168484526026\n",
      "  (25, 7)\t0.000195963158926\n",
      "  (26, 0)\t0.000755857898715\n",
      "  (26, 1)\t0.117578878448\n",
      "  (26, 2)\t0.0903623452124\n",
      "  (26, 3)\t0.00158506909562\n",
      "  (26, 4)\t0.129858253315\n",
      "  (26, 5)\t0.0506609711503\n",
      "  (26, 6)\t0.0185332978629\n",
      "  (26, 7)\t0.000195963158926\n",
      "  (27, 0)\t0.000755857898715\n",
      "  (27, 1)\t0.0961228787316\n",
      "  (27, 2)\t0.0903623452124\n",
      "  (27, 3)\t0.0412117964861\n",
      "  (27, 4)\t0.0657834572714\n",
      "  (27, 5)\t0.0302331602026\n",
      "  (27, 6)\t0.0522302030682\n",
      "  (27, 7)\t0.000195963158926\n",
      "  (28, 0)\t0.000755857898715\n",
      "  (28, 1)\t0.104705278618\n",
      "  (28, 2)\t0.0228017132779\n",
      "  (28, 3)\t0.033286451008\n",
      "  (28, 4)\t0.129858253315\n",
      "  (28, 5)\t0.0588320955293\n",
      "  (28, 6)\t0.0606544293695\n",
      "  (28, 7)\t0.000195963158926\n",
      "  (29, 0)\t0.00453514739229\n",
      "  (29, 1)\t0.0961228787316\n",
      "  (29, 2)\t0.0312467922697\n",
      "  (29, 3)\t0.033286451008\n",
      "  (29, 4)\t0.099956681828\n",
      "  (29, 5)\t0.0588320955293\n",
      "  (29, 6)\t0.0395938636162\n",
      "  (29, 7)\t0.000195963158926\n",
      "  (30, 0)\t0.000755857898715\n",
      "  (30, 1)\t0.117578878448\n",
      "  (30, 2)\t0.0439144107574\n",
      "  (30, 3)\t0.037249123747\n",
      "  (30, 4)\t0.00170866122783\n",
      "  (30, 5)\t0.0506609711503\n",
      "  (30, 6)\t0.0648665425202\n",
      "  (30, 7)\t0.000195963158926\n",
      "  (31, 0)\t0.000755857898715\n",
      "  (31, 1)\t0.0832492789015\n",
      "  (31, 2)\t0.0312467922697\n",
      "  (31, 3)\t0.033286451008\n",
      "  (31, 4)\t0.129858253315\n",
      "  (31, 5)\t0.0506609711503\n",
      "  (31, 6)\t0.0648665425202\n",
      "  (31, 7)\t0.000195963158926\n",
      "  (32, 0)\t0.000755857898715\n",
      "  (32, 1)\t0.0703756790713\n",
      "  (32, 2)\t0.0819172662206\n",
      "  (32, 3)\t0.033286451008\n",
      "  (32, 4)\t0.129858253315\n",
      "  (32, 5)\t0.0302331602026\n",
      "  (32, 6)\t0.0522302030682\n",
      "  (32, 7)\t0.000195963158926\n",
      "  (33, 0)\t0.000755857898715\n",
      "  (33, 1)\t0.104705278618\n",
      "  (33, 2)\t0.0819172662206\n",
      "  (33, 3)\t0.037249123747\n",
      "  (33, 4)\t0.129858253315\n",
      "  (33, 5)\t0.0302331602026\n",
      "  (33, 6)\t0.0522302030682\n",
      "  (33, 7)\t0.000195963158926\n",
      "  (34, 0)\t0.000755857898715\n",
      "  (34, 1)\t0.0832492789015\n",
      "  (34, 2)\t0.0903623452124\n",
      "  (34, 3)\t0.00158506909562\n",
      "  (34, 4)\t0.0657834572714\n",
      "  (34, 5)\t0.00163422487582\n",
      "  (34, 6)\t0.0395938636162\n",
      "  (34, 7)\t0.000195963158926\n",
      "  (35, 0)\t0.000755857898715\n",
      "  (35, 1)\t0.0961228787316\n",
      "  (35, 2)\t0.0312467922697\n",
      "  (35, 3)\t0.0412117964861\n",
      "  (35, 4)\t0.129858253315\n",
      "  (35, 5)\t0.0302331602026\n",
      "  (35, 6)\t0.00168484526026\n",
      "  (35, 7)\t0.000195963158926\n",
      "  (36, 0)\t0.000755857898715\n",
      "  (36, 1)\t0.0961228787316\n",
      "  (36, 2)\t0.0312467922697\n",
      "  (36, 3)\t0.033286451008\n",
      "  (36, 4)\t0.099956681828\n",
      "  (36, 5)\t0.00163422487582\n",
      "  (36, 6)\t0.0522302030682\n",
      "  (36, 7)\t0.000195963158926\n",
      "  (37, 0)\t0.000755857898715\n",
      "  (37, 1)\t0.0832492789015\n",
      "  (37, 2)\t0.0903623452124\n",
      "  (37, 3)\t0.00158506909562\n",
      "  (37, 4)\t0.129858253315\n",
      "  (37, 5)\t0.0302331602026\n",
      "  (37, 6)\t0.0522302030682\n",
      "  (37, 7)\t0.000195963158926\n",
      "  (38, 0)\t0.000755857898715\n",
      "  (38, 1)\t0.0961228787316\n",
      "  (38, 2)\t0.0312467922697\n",
      "  (38, 3)\t0.033286451008\n",
      "  (38, 4)\t0.099956681828\n",
      "  (38, 5)\t0.0588320955293\n",
      "  (38, 6)\t0.0648665425202\n",
      "  (38, 7)\t0.000195963158926\n",
      "  (39, 0)\t0.000755857898715\n",
      "  (39, 1)\t0.104705278618\n",
      "  (39, 2)\t0.0439144107574\n",
      "  (39, 3)\t0.033286451008\n",
      "  (39, 4)\t0.129858253315\n",
      "  (39, 5)\t0.0588320955293\n",
      "  (39, 6)\t0.0648665425202\n",
      "  (39, 7)\t0.000195963158926\n",
      "  (40, 0)\t0.000755857898715\n",
      "  (40, 1)\t0.117578878448\n",
      "  (40, 2)\t0.0228017132779\n",
      "  (40, 3)\t0.037249123747\n",
      "  (40, 4)\t0.129858253315\n",
      "  (40, 5)\t0.0588320955293\n",
      "  (40, 6)\t0.0606544293695\n",
      "  (40, 7)\t0.000195963158926\n",
      "  (41, 0)\t0.000755857898715\n",
      "  (41, 1)\t0.0961228787316\n",
      "  (41, 2)\t0.0439144107574\n",
      "  (41, 3)\t0.037249123747\n",
      "  (41, 4)\t0.129858253315\n",
      "  (41, 5)\t0.00163422487582\n",
      "  (41, 6)\t0.0522302030682\n",
      "  (41, 7)\t0.000195963158926\n",
      "  (42, 0)\t0.000755857898715\n",
      "  (42, 1)\t0.117578878448\n",
      "  (42, 2)\t0.0903623452124\n",
      "  (42, 3)\t0.00158506909562\n",
      "  (42, 4)\t0.0316102327148\n",
      "  (42, 5)\t0.0302331602026\n",
      "  (42, 6)\t0.0522302030682\n",
      "  (42, 7)\t0.000195963158926\n",
      "  (43, 0)\t0.00453514739229\n",
      "  (43, 1)\t0.0832492789015\n",
      "  (43, 2)\t0.0439144107574\n",
      "  (43, 3)\t0.0412117964861\n",
      "  (43, 4)\t0.0914133756889\n",
      "  (43, 5)\t0.0506609711503\n",
      "  (43, 6)\t0.0606544293695\n",
      "  (43, 7)\t0.000195963158926\n",
      "  (44, 0)\t0.000755857898715\n",
      "  (44, 1)\t0.00171647997735\n",
      "  (44, 2)\t0.00168901579836\n",
      "  (44, 3)\t0.00158506909562\n",
      "  (44, 4)\t0.00170866122783\n",
      "  (44, 5)\t0.00163422487582\n",
      "  (44, 6)\t0.00168484526026\n",
      "  (44, 7)\t0.000195963158926\n",
      "  (45, 0)\t0.000755857898715\n",
      "  (45, 1)\t0.104705278618\n",
      "  (45, 2)\t0.0439144107574\n",
      "  (45, 3)\t0.033286451008\n",
      "  (45, 4)\t0.0657834572714\n",
      "  (45, 5)\t0.0138909114444\n",
      "  (45, 6)\t0.0648665425202\n",
      "  (45, 7)\t0.000195963158926\n",
      "  (46, 0)\t0.000755857898715\n",
      "  (46, 1)\t0.104705278618\n",
      "  (46, 2)\t0.00168901579836\n",
      "  (46, 3)\t0.00158506909562\n",
      "  (46, 4)\t0.099956681828\n",
      "  (46, 5)\t0.00163422487582\n",
      "  (46, 6)\t0.0606544293695\n",
      "  (46, 7)\t0.000195963158926\n",
      "  (47, 0)\t0.000755857898715\n",
      "  (47, 1)\t0.0961228787316\n",
      "  (47, 2)\t0.0439144107574\n",
      "  (47, 3)\t0.037249123747\n",
      "  (47, 4)\t0.099956681828\n",
      "  (47, 5)\t0.0506609711503\n",
      "  (47, 6)\t0.0185332978629\n",
      "  (47, 7)\t0.000195963158926\n",
      "  (48, 0)\t0.000755857898715\n",
      "  (48, 1)\t0.0961228787316\n",
      "  (48, 2)\t0.0312467922697\n",
      "  (48, 3)\t0.00158506909562\n",
      "  (48, 4)\t0.129858253315\n",
      "  (48, 5)\t0.0588320955293\n",
      "  (48, 6)\t0.0269575241642\n",
      "  (48, 7)\t0.000195963158926\n",
      "  (49, 0)\t0.000755857898715\n",
      "  (49, 1)\t0.117578878448\n",
      "  (49, 2)\t0.0903623452124\n",
      "  (49, 3)\t0.00158506909562\n",
      "  (49, 4)\t0.129858253315\n",
      "  (49, 5)\t0.0506609711503\n",
      "  (49, 6)\t0.0185332978629\n",
      "  (49, 7)\t0.000195963158926\n",
      "  (50, 0)\t0.000755857898715\n",
      "  (50, 1)\t0.0832492789015\n",
      "  (50, 2)\t0.0903623452124\n",
      "  (50, 3)\t0.037249123747\n",
      "  (50, 4)\t0.099956681828\n",
      "  (50, 5)\t0.0506609711503\n",
      "  (50, 6)\t0.0185332978629\n",
      "  (50, 7)\t0.000195963158926\n",
      "  (51, 0)\t0.000755857898715\n",
      "  (51, 1)\t0.0703756790713\n",
      "  (51, 2)\t0.0439144107574\n",
      "  (51, 3)\t0.033286451008\n",
      "  (51, 4)\t0.129858253315\n",
      "  (51, 5)\t0.0302331602026\n",
      "  (51, 6)\t0.00168484526026\n",
      "  (51, 7)\t0.000195963158926\n",
      "  (52, 0)\t0.000755857898715\n",
      "  (52, 1)\t0.117578878448\n",
      "  (52, 2)\t0.0312467922697\n",
      "  (52, 3)\t0.033286451008\n",
      "  (52, 4)\t0.099956681828\n",
      "  (52, 5)\t0.0506609711503\n",
      "  (52, 6)\t0.00168484526026\n",
      "  (52, 7)\t0.000195963158926\n",
      "  (53, 0)\t0.000755857898715\n",
      "  (53, 1)\t0.104705278618\n",
      "  (53, 2)\t0.0819172662206\n",
      "  (53, 3)\t0.037249123747\n",
      "  (53, 4)\t0.129858253315\n",
      "  (53, 5)\t0.0302331602026\n",
      "  (53, 6)\t0.0522302030682\n",
      "  (53, 7)\t0.000195963158926\n",
      "  (54, 0)\t0.000755857898715\n",
      "  (54, 1)\t0.104705278618\n",
      "  (54, 2)\t0.0903623452124\n",
      "  (54, 3)\t0.037249123747\n",
      "  (54, 4)\t0.0657834572714\n",
      "  (54, 5)\t0.0588320955293\n",
      "  (54, 6)\t0.0522302030682\n",
      "  (54, 7)\t0.000195963158926\n",
      "  (55, 0)\t0.000755857898715\n",
      "  (55, 1)\t0.00171647997735\n",
      "  (55, 2)\t0.00168901579836\n",
      "  (55, 3)\t0.00158506909562\n",
      "  (55, 4)\t0.00170866122783\n",
      "  (55, 5)\t0.00163422487582\n",
      "  (55, 6)\t0.00168484526026\n",
      "  (55, 7)\t0.000195963158926\n",
      "  (56, 0)\t0.000755857898715\n",
      "  (56, 1)\t0.104705278618\n",
      "  (56, 2)\t0.0903623452124\n",
      "  (56, 3)\t0.0412117964861\n",
      "  (56, 4)\t0.0316102327148\n",
      "  (56, 5)\t0.0506609711503\n",
      "  (56, 6)\t0.0606544293695\n",
      "  (56, 7)\t0.000195963158926\n",
      "  (57, 0)\t0.000755857898715\n",
      "  (57, 1)\t0.0832492789015\n",
      "  (57, 2)\t0.0903623452124\n",
      "  (57, 3)\t0.037249123747\n",
      "  (57, 4)\t0.099956681828\n",
      "  (57, 5)\t0.0506609711503\n",
      "  (57, 6)\t0.0185332978629\n",
      "  (57, 7)\t0.000195963158926\n",
      "  (58, 0)\t0.000755857898715\n",
      "  (58, 1)\t0.104705278618\n",
      "  (58, 2)\t0.0819172662206\n",
      "  (58, 3)\t0.00158506909562\n",
      "  (58, 4)\t0.0316102327148\n",
      "  (58, 5)\t0.0588320955293\n",
      "  (58, 6)\t0.0522302030682\n",
      "  (58, 7)\t0.000195963158926\n",
      "  (59, 0)\t0.000755857898715\n",
      "  (59, 1)\t0.00171647997735\n",
      "  (59, 2)\t0.00168901579836\n",
      "  (59, 3)\t0.00158506909562\n",
      "  (59, 4)\t0.00170866122783\n",
      "  (59, 5)\t0.00163422487582\n",
      "  (59, 6)\t0.00168484526026\n",
      "  (59, 7)\t0.000195963158926\n",
      "  (60, 0)\t0.000755857898715\n",
      "  (60, 1)\t0.117578878448\n",
      "  (60, 2)\t0.0439144107574\n",
      "  (60, 3)\t0.00158506909562\n",
      "  (60, 4)\t0.129858253315\n",
      "  (60, 5)\t0.00163422487582\n",
      "  (60, 6)\t0.0606544293695\n",
      "  (60, 7)\t0.000195963158926\n",
      "  (61, 0)\t0.000755857898715\n",
      "  (61, 1)\t0.0703756790713\n",
      "  (61, 2)\t0.0312467922697\n",
      "  (61, 3)\t0.037249123747\n",
      "  (61, 4)\t0.0657834572714\n",
      "  (61, 5)\t0.0588320955293\n",
      "  (61, 6)\t0.0395938636162\n",
      "  (61, 7)\t0.000195963158926\n",
      "  (62, 0)\t0.000755857898715\n",
      "  (62, 1)\t0.117578878448\n",
      "  (62, 2)\t0.0439144107574\n",
      "  (62, 3)\t0.033286451008\n",
      "  (62, 4)\t0.099956681828\n",
      "  (62, 5)\t0.00163422487582\n",
      "  (62, 6)\t0.0353817504655\n",
      "  (62, 7)\t0.000195963158926\n",
      "  (63, 0)\t0.000755857898715\n",
      "  (63, 1)\t0.0961228787316\n",
      "  (63, 2)\t0.00168901579836\n",
      "  (63, 3)\t0.033286451008\n",
      "  (63, 4)\t0.0657834572714\n",
      "  (63, 5)\t0.0588320955293\n",
      "  (63, 6)\t0.0395938636162\n",
      "  (63, 7)\t0.000195963158926\n",
      "  (64, 0)\t0.000755857898715\n",
      "  (64, 1)\t0.00171647997735\n",
      "  (64, 2)\t0.00168901579836\n",
      "  (64, 3)\t0.00158506909562\n",
      "  (64, 4)\t0.00170866122783\n",
      "  (64, 5)\t0.00163422487582\n",
      "  (64, 6)\t0.00168484526026\n",
      "  (64, 7)\t0.000195963158926\n",
      "  (65, 0)\t0.000755857898715\n",
      "  (65, 1)\t0.0961228787316\n",
      "  (65, 2)\t0.0312467922697\n",
      "  (65, 3)\t0.037249123747\n",
      "  (65, 4)\t0.129858253315\n",
      "  (65, 5)\t0.0302331602026\n",
      "  (65, 6)\t0.00168484526026\n",
      "  (65, 7)\t0.000195963158926\n",
      "  (66, 0)\t0.000755857898715\n",
      "  (66, 1)\t0.117578878448\n",
      "  (66, 2)\t0.0819172662206\n",
      "  (66, 3)\t0.0412117964861\n",
      "  (66, 4)\t0.129858253315\n",
      "  (66, 5)\t0.0506609711503\n",
      "  (66, 6)\t0.0185332978629\n",
      "  (66, 7)\t0.000195963158926\n",
      "  (67, 0)\t0.000755857898715\n",
      "  (67, 1)\t0.0961228787316\n",
      "  (67, 2)\t0.0819172662206\n",
      "  (67, 3)\t0.037249123747\n",
      "  (67, 4)\t0.099956681828\n",
      "  (67, 5)\t0.0302331602026\n",
      "  (67, 6)\t0.0606544293695\n",
      "  (67, 7)\t0.000195963158926\n",
      "  (68, 0)\t0.000755857898715\n",
      "  (68, 1)\t0.117578878448\n",
      "  (68, 2)\t0.00168901579836\n",
      "  (68, 3)\t0.00158506909562\n",
      "  (68, 4)\t0.099956681828\n",
      "  (68, 5)\t0.00163422487582\n",
      "  (68, 6)\t0.0522302030682\n",
      "  (68, 7)\t0.000195963158926\n",
      "  (69, 0)\t0.000755857898715\n",
      "  (69, 1)\t0.0832492789015\n",
      "  (69, 2)\t0.0903623452124\n",
      "  (69, 3)\t0.00158506909562\n",
      "  (69, 4)\t0.0316102327148\n",
      "  (69, 5)\t0.00163422487582\n",
      "  (69, 6)\t0.00168484526026\n",
      "  (69, 7)\t0.000195963158926\n",
      "  (70, 0)\t0.000755857898715\n",
      "  (70, 1)\t0.00171647997735\n",
      "  (70, 2)\t0.00168901579836\n",
      "  (70, 3)\t0.00158506909562\n",
      "  (70, 4)\t0.00170866122783\n",
      "  (70, 5)\t0.00163422487582\n",
      "  (70, 6)\t0.00168484526026\n",
      "  (70, 7)\t0.000195963158926\n",
      "  (71, 0)\t0.000755857898715\n",
      "  (71, 1)\t0.0832492789015\n",
      "  (71, 2)\t0.00168901579836\n",
      "  (71, 3)\t0.033286451008\n",
      "  (71, 4)\t0.0914133756889\n",
      "  (71, 5)\t0.0588320955293\n",
      "  (71, 6)\t0.0353817504655\n",
      "  (71, 7)\t0.000195963158926\n",
      "  (72, 0)\t0.000755857898715\n",
      "  (72, 1)\t0.104705278618\n",
      "  (72, 2)\t0.0903623452124\n",
      "  (72, 3)\t0.00158506909562\n",
      "  (72, 4)\t0.099956681828\n",
      "  (72, 5)\t0.0506609711503\n",
      "  (72, 6)\t0.0185332978629\n",
      "  (72, 7)\t0.000195963158926\n",
      "  (73, 0)\t0.000755857898715\n",
      "  (73, 1)\t0.0832492789015\n",
      "  (73, 2)\t0.0439144107574\n",
      "  (73, 3)\t0.00158506909562\n",
      "  (73, 4)\t0.099956681828\n",
      "  (73, 5)\t0.0506609711503\n",
      "  (73, 6)\t0.0606544293695\n",
      "  (73, 7)\t0.000195963158926\n",
      "  (74, 0)\t0.000755857898715\n",
      "  (74, 1)\t0.104705278618\n",
      "  (74, 2)\t0.0439144107574\n",
      "  (74, 3)\t0.00158506909562\n",
      "  (74, 4)\t0.0316102327148\n",
      "  (74, 5)\t0.00163422487582\n",
      "  (74, 6)\t0.00168484526026\n",
      "  (74, 7)\t0.000195963158926\n",
      "  (75, 0)\t0.000755857898715\n",
      "  (75, 1)\t0.0961228787316\n",
      "  (75, 2)\t0.0903623452124\n",
      "  (75, 3)\t0.033286451008\n",
      "  (75, 4)\t0.0914133756889\n",
      "  (75, 5)\t0.0302331602026\n",
      "  (75, 6)\t0.0606544293695\n",
      "  (75, 7)\t0.000195963158926\n",
      "  (76, 0)\t0.000755857898715\n",
      "  (76, 1)\t0.117578878448\n",
      "  (76, 2)\t0.0439144107574\n",
      "  (76, 3)\t0.0412117964861\n",
      "  (76, 4)\t0.0914133756889\n",
      "  (76, 5)\t0.0302331602026\n",
      "  (76, 6)\t0.0522302030682\n",
      "  (76, 7)\t0.000195963158926\n",
      "  (77, 0)\t0.000755857898715\n",
      "  (77, 1)\t0.117578878448\n",
      "  (77, 2)\t0.00168901579836\n",
      "  (77, 3)\t0.037249123747\n",
      "  (77, 4)\t0.099956681828\n",
      "  (77, 5)\t0.00980534925489\n",
      "  (77, 6)\t0.0185332978629\n",
      "  (77, 7)\t0.000195963158926\n",
      "  (78, 0)\t0.000755857898715\n",
      "  (78, 1)\t0.117578878448\n",
      "  (78, 2)\t0.0228017132779\n",
      "  (78, 3)\t0.037249123747\n",
      "  (78, 4)\t0.129858253315\n",
      "  (78, 5)\t0.0588320955293\n",
      "  (78, 6)\t0.0606544293695\n",
      "  (78, 7)\t0.000195963158926\n",
      "  (79, 0)\t0.000755857898715\n",
      "  (79, 1)\t0.0961228787316\n",
      "  (79, 2)\t0.0819172662206\n",
      "  (79, 3)\t0.00158506909562\n",
      "  (79, 4)\t0.099956681828\n",
      "  (79, 5)\t0.0302331602026\n",
      "  (79, 6)\t0.0606544293695\n",
      "  (79, 7)\t0.000195963158926\n",
      "  (80, 0)\t0.000755857898715\n",
      "  (80, 1)\t0.0703756790713\n",
      "  (80, 2)\t0.0312467922697\n",
      "  (80, 3)\t0.037249123747\n",
      "  (80, 4)\t0.0316102327148\n",
      "  (80, 5)\t0.0506609711503\n",
      "  (80, 6)\t0.00168484526026\n",
      "  (80, 7)\t0.000195963158926\n",
      "  (81, 0)\t0.000755857898715\n",
      "  (81, 1)\t0.00171647997735\n",
      "  (81, 2)\t0.00168901579836\n",
      "  (81, 3)\t0.00158506909562\n",
      "  (81, 4)\t0.00170866122783\n",
      "  (81, 5)\t0.00163422487582\n",
      "  (81, 6)\t0.00168484526026\n",
      "  (81, 7)\t0.000195963158926\n",
      "  (82, 0)\t0.000755857898715\n",
      "  (82, 1)\t0.0832492789015\n",
      "  (82, 2)\t0.0819172662206\n",
      "  (82, 3)\t0.00158506909562\n",
      "  (82, 4)\t0.099956681828\n",
      "  (82, 5)\t0.0302331602026\n",
      "  (82, 6)\t0.0185332978629\n",
      "  (82, 7)\t0.000195963158926\n",
      "  (83, 0)\t0.000755857898715\n",
      "  (83, 1)\t0.0961228787316\n",
      "  (83, 2)\t0.0312467922697\n",
      "  (83, 3)\t0.033286451008\n",
      "  (83, 4)\t0.0914133756889\n",
      "  (83, 5)\t0.0588320955293\n",
      "  (83, 6)\t0.0353817504655\n",
      "  (83, 7)\t0.000195963158926\n",
      "  (84, 0)\t0.000755857898715\n",
      "  (84, 1)\t0.104705278618\n",
      "  (84, 2)\t0.0903623452124\n",
      "  (84, 3)\t0.0412117964861\n",
      "  (84, 4)\t0.099956681828\n",
      "  (84, 5)\t0.0588320955293\n",
      "  (84, 6)\t0.00168484526026\n",
      "  (84, 7)\t0.000195963158926\n",
      "  (85, 0)\t0.000755857898715\n",
      "  (85, 1)\t0.0832492789015\n",
      "  (85, 2)\t0.0819172662206\n",
      "  (85, 3)\t0.00158506909562\n",
      "  (85, 4)\t0.0914133756889\n",
      "  (85, 5)\t0.0588320955293\n",
      "  (85, 6)\t0.0522302030682\n",
      "  (85, 7)\t0.000195963158926\n",
      "  (86, 0)\t0.000755857898715\n",
      "  (86, 1)\t0.0961228787316\n",
      "  (86, 2)\t0.0819172662206\n",
      "  (86, 3)\t0.00158506909562\n",
      "  (86, 4)\t0.099956681828\n",
      "  (86, 5)\t0.0588320955293\n",
      "  (86, 6)\t0.0606544293695\n",
      "  (86, 7)\t0.000195963158926\n",
      "  (87, 0)\t0.000755857898715\n",
      "  (87, 1)\t0.117578878448\n",
      "  (87, 2)\t0.0903623452124\n",
      "  (87, 3)\t0.037249123747\n",
      "  (87, 4)\t0.0914133756889\n",
      "  (87, 5)\t0.0302331602026\n",
      "  (87, 6)\t0.0395938636162\n",
      "  (87, 7)\t0.000195963158926\n",
      "  (88, 0)\t0.000755857898715\n",
      "  (88, 1)\t0.104705278618\n",
      "  (88, 2)\t0.0312467922697\n",
      "  (88, 3)\t0.033286451008\n",
      "  (88, 4)\t0.099956681828\n",
      "  (88, 5)\t0.00163422487582\n",
      "  (88, 6)\t0.0522302030682\n",
      "  (88, 7)\t0.000195963158926\n",
      "  (89, 0)\t0.000755857898715\n",
      "  (89, 1)\t0.0832492789015\n",
      "  (89, 2)\t0.00168901579836\n",
      "  (89, 3)\t0.00158506909562\n",
      "  (89, 4)\t0.0316102327148\n",
      "  (89, 5)\t0.00163422487582\n",
      "  (89, 6)\t0.0522302030682\n",
      "  (89, 7)\t0.000195963158926\n",
      "  (90, 0)\t0.000755857898715\n",
      "  (90, 1)\t0.117578878448\n",
      "  (90, 2)\t0.0312467922697\n",
      "  (90, 3)\t0.00158506909562\n",
      "  (90, 4)\t0.0914133756889\n",
      "  (90, 5)\t0.00980534925489\n",
      "  (90, 6)\t0.0648665425202\n",
      "  (90, 7)\t0.000195963158926\n",
      "  (91, 0)\t0.00453514739229\n",
      "  (91, 1)\t0.0961228787316\n",
      "  (91, 2)\t0.0819172662206\n",
      "  (91, 3)\t0.033286451008\n",
      "  (91, 4)\t0.129858253315\n",
      "  (91, 5)\t0.0302331602026\n",
      "  (91, 6)\t0.0522302030682\n",
      "  (91, 7)\t0.000195963158926\n",
      "  (92, 0)\t0.000755857898715\n",
      "  (92, 1)\t0.117578878448\n",
      "  (92, 2)\t0.0439144107574\n",
      "  (92, 3)\t0.00158506909562\n",
      "  (92, 4)\t0.0316102327148\n",
      "  (92, 5)\t0.00163422487582\n",
      "  (92, 6)\t0.0353817504655\n",
      "  (92, 7)\t0.000195963158926\n",
      "  (93, 0)\t0.000755857898715\n",
      "  (93, 1)\t0.00171647997735\n",
      "  (93, 2)\t0.0903623452124\n",
      "  (93, 3)\t0.00158506909562\n",
      "  (93, 4)\t0.0914133756889\n",
      "  (93, 5)\t0.0302331602026\n",
      "  (93, 6)\t0.0648665425202\n",
      "  (93, 7)\t0.000195963158926\n",
      "  (94, 0)\t0.000755857898715\n",
      "  (94, 1)\t0.117578878448\n",
      "  (94, 2)\t0.0228017132779\n",
      "  (94, 3)\t0.037249123747\n",
      "  (94, 4)\t0.129858253315\n",
      "  (94, 5)\t0.0588320955293\n",
      "  (94, 6)\t0.0606544293695\n",
      "  (94, 7)\t0.000195963158926\n",
      "  (95, 0)\t0.000755857898715\n",
      "  (95, 1)\t0.117578878448\n",
      "  (95, 2)\t0.0903623452124\n",
      "  (95, 3)\t0.00158506909562\n",
      "  (95, 4)\t0.129858253315\n",
      "  (95, 5)\t0.0506609711503\n",
      "  (95, 6)\t0.0185332978629\n",
      "  (95, 7)\t0.000195963158926\n",
      "  (96, 0)\t0.000755857898715\n",
      "  (96, 1)\t0.0188812797508\n",
      "  (96, 2)\t0.0819172662206\n",
      "  (96, 3)\t0.033286451008\n",
      "  (96, 4)\t0.0914133756889\n",
      "  (96, 5)\t0.0302331602026\n",
      "  (96, 6)\t0.0606544293695\n",
      "  (96, 7)\t0.000195963158926\n",
      "  (97, 0)\t0.000755857898715\n",
      "  (97, 1)\t0.104705278618\n",
      "  (97, 2)\t0.0312467922697\n",
      "  (97, 3)\t0.033286451008\n",
      "  (97, 4)\t0.129858253315\n",
      "  (97, 5)\t0.00163422487582\n",
      "  (97, 6)\t0.0648665425202\n",
      "  (97, 7)\t0.000195963158926\n",
      "  (98, 0)\t0.000755857898715\n",
      "  (98, 1)\t0.0832492789015\n",
      "  (98, 2)\t0.0819172662206\n",
      "  (98, 3)\t0.0412117964861\n",
      "  (98, 4)\t0.099956681828\n",
      "  (98, 5)\t0.0302331602026\n",
      "  (98, 6)\t0.0606544293695\n",
      "  (98, 7)\t0.000195963158926\n",
      "  (99, 0)\t0.000755857898715\n",
      "  (99, 1)\t0.0961228787316\n",
      "  (99, 2)\t0.0439144107574\n",
      "  (99, 3)\t0.037249123747\n",
      "  (99, 4)\t0.0316102327148\n",
      "  (99, 5)\t0.0302331602026\n",
      "  (99, 6)\t0.0185332978629\n",
      "  (99, 7)\t0.000195963158926\n",
      "  (100, 0)\t0.000755857898715\n",
      "  (100, 1)\t0.0832492789015\n",
      "  (100, 2)\t0.0903623452124\n",
      "  (100, 3)\t0.0412117964861\n",
      "  (100, 4)\t0.129858253315\n",
      "  (100, 5)\t0.0588320955293\n",
      "  (100, 6)\t0.00168484526026\n",
      "  (100, 7)\t0.000195963158926\n",
      "  (101, 0)\t0.000755857898715\n",
      "  (101, 1)\t0.0832492789015\n",
      "  (101, 2)\t0.0439144107574\n",
      "  (101, 3)\t0.037249123747\n",
      "  (101, 4)\t0.099956681828\n",
      "  (101, 5)\t0.00163422487582\n",
      "  (101, 6)\t0.00168484526026\n",
      "  (101, 7)\t0.000195963158926\n",
      "  (102, 0)\t0.000755857898715\n",
      "  (102, 1)\t0.104705278618\n",
      "  (102, 2)\t0.0439144107574\n",
      "  (102, 3)\t0.033286451008\n",
      "  (102, 4)\t0.0657834572714\n",
      "  (102, 5)\t0.0138909114444\n",
      "  (102, 6)\t0.0648665425202\n",
      "  (102, 7)\t0.000195963158926\n",
      "  (103, 0)\t0.000755857898715\n",
      "  (103, 1)\t0.0832492789015\n",
      "  (103, 2)\t0.0312467922697\n",
      "  (103, 3)\t0.00158506909562\n",
      "  (103, 4)\t0.0657834572714\n",
      "  (103, 5)\t0.0302331602026\n",
      "  (103, 6)\t0.0353817504655\n",
      "  (103, 7)\t0.000195963158926\n",
      "  (104, 0)\t0.000755857898715\n",
      "  (104, 1)\t0.0961228787316\n",
      "  (104, 2)\t0.0439144107574\n",
      "  (104, 3)\t0.033286451008\n",
      "  (104, 4)\t0.0316102327148\n",
      "  (104, 5)\t0.00163422487582\n",
      "  (104, 6)\t0.0648665425202\n",
      "  (104, 7)\t0.000195963158926\n",
      "  (105, 0)\t0.000755857898715\n",
      "  (105, 1)\t0.104705278618\n",
      "  (105, 2)\t0.0903623452124\n",
      "  (105, 3)\t0.00158506909562\n",
      "  (105, 4)\t0.0657834572714\n",
      "  (105, 5)\t0.0588320955293\n",
      "  (105, 6)\t0.0606544293695\n",
      "  (105, 7)\t0.000195963158926\n",
      "  (106, 0)\t0.000755857898715\n",
      "  (106, 1)\t0.0832492789015\n",
      "  (106, 2)\t0.0903623452124\n",
      "  (106, 3)\t0.033286451008\n",
      "  (106, 4)\t0.0914133756889\n",
      "  (106, 5)\t0.0506609711503\n",
      "  (106, 6)\t0.0606544293695\n",
      "  (106, 7)\t0.000195963158926\n",
      "  (107, 0)\t0.000755857898715\n",
      "  (107, 1)\t0.0832492789015\n",
      "  (107, 2)\t0.0903623452124\n",
      "  (107, 3)\t0.00158506909562\n",
      "  (107, 4)\t0.129858253315\n",
      "  (107, 5)\t0.00163422487582\n",
      "  (107, 6)\t0.0648665425202\n",
      "  (107, 7)\t0.000195963158926\n",
      "  (108, 0)\t0.000755857898715\n",
      "  (108, 1)\t0.117578878448\n",
      "  (108, 2)\t0.0903623452124\n",
      "  (108, 3)\t0.033286451008\n",
      "  (108, 4)\t0.099956681828\n",
      "  (108, 5)\t0.00163422487582\n",
      "  (108, 6)\t0.0269575241642\n",
      "  (108, 7)\t0.000195963158926\n",
      "  (109, 0)\t0.000755857898715\n",
      "  (109, 1)\t0.0703756790713\n",
      "  (109, 2)\t0.0439144107574\n",
      "  (109, 3)\t0.033286451008\n",
      "  (109, 4)\t0.129858253315\n",
      "  (109, 5)\t0.0588320955293\n",
      "  (109, 6)\t0.0606544293695\n",
      "  (109, 7)\t0.000195963158926\n",
      "  (110, 0)\t0.000755857898715\n",
      "  (110, 1)\t0.0961228787316\n",
      "  (110, 2)\t0.0312467922697\n",
      "  (110, 3)\t0.00158506909562\n",
      "  (110, 4)\t0.099956681828\n",
      "  (110, 5)\t0.00163422487582\n",
      "  (110, 6)\t0.0606544293695\n",
      "  (110, 7)\t0.000195963158926\n",
      "  (111, 0)\t0.000755857898715\n",
      "  (111, 1)\t0.0961228787316\n",
      "  (111, 2)\t0.0439144107574\n",
      "  (111, 3)\t0.037249123747\n",
      "  (111, 4)\t0.129858253315\n",
      "  (111, 5)\t0.00163422487582\n",
      "  (111, 6)\t0.0522302030682\n",
      "  (111, 7)\t0.000195963158926\n",
      "  (112, 0)\t0.000755857898715\n",
      "  (112, 1)\t0.0832492789015\n",
      "  (112, 2)\t0.0439144107574\n",
      "  (112, 3)\t0.037249123747\n",
      "  (112, 4)\t0.129858253315\n",
      "  (112, 5)\t0.00163422487582\n",
      "  (112, 6)\t0.00168484526026\n",
      "  (112, 7)\t0.000195963158926\n",
      "  (113, 0)\t0.000755857898715\n",
      "  (113, 1)\t0.104705278618\n",
      "  (113, 2)\t0.0439144107574\n",
      "  (113, 3)\t0.00158506909562\n",
      "  (113, 4)\t0.0316102327148\n",
      "  (113, 5)\t0.00163422487582\n",
      "  (113, 6)\t0.0606544293695\n",
      "  (113, 7)\t0.000195963158926\n",
      "  (114, 0)\t0.00453514739229\n",
      "  (114, 1)\t0.104705278618\n",
      "  (114, 2)\t0.0903623452124\n",
      "  (114, 3)\t0.0412117964861\n",
      "  (114, 4)\t0.0914133756889\n",
      "  (114, 5)\t0.0506609711503\n",
      "  (114, 6)\t0.0606544293695\n",
      "  (114, 7)\t0.000195963158926\n",
      "  (115, 0)\t0.000755857898715\n",
      "  (115, 1)\t0.104705278618\n",
      "  (115, 2)\t0.0312467922697\n",
      "  (115, 3)\t0.037249123747\n",
      "  (115, 4)\t0.129858253315\n",
      "  (115, 5)\t0.00163422487582\n",
      "  (115, 6)\t0.0606544293695\n",
      "  (115, 7)\t0.000195963158926\n",
      "  (116, 0)\t0.000755857898715\n",
      "  (116, 1)\t0.0703756790713\n",
      "  (116, 2)\t0.0439144107574\n",
      "  (116, 3)\t0.033286451008\n",
      "  (116, 4)\t0.129858253315\n",
      "  (116, 5)\t0.0302331602026\n",
      "  (116, 6)\t0.00168484526026\n",
      "  (116, 7)\t0.000195963158926\n",
      "  (117, 0)\t0.000755857898715\n",
      "  (117, 1)\t0.0188812797508\n",
      "  (117, 2)\t0.0439144107574\n",
      "  (117, 3)\t0.037249123747\n",
      "  (117, 4)\t0.129858253315\n",
      "  (117, 5)\t0.0588320955293\n",
      "  (117, 6)\t0.0185332978629\n",
      "  (117, 7)\t0.000195963158926\n",
      "  (118, 0)\t0.000755857898715\n",
      "  (118, 1)\t0.0961228787316\n",
      "  (118, 2)\t0.0819172662206\n",
      "  (118, 3)\t0.00158506909562\n",
      "  (118, 4)\t0.099956681828\n",
      "  (118, 5)\t0.0588320955293\n",
      "  (118, 6)\t0.0522302030682\n",
      "  (118, 7)\t0.000195963158926\n",
      "  (119, 0)\t0.000755857898715\n",
      "  (119, 1)\t0.104705278618\n",
      "  (119, 2)\t0.0819172662206\n",
      "  (119, 3)\t0.037249123747\n",
      "  (119, 4)\t0.129858253315\n",
      "  (119, 5)\t0.0302331602026\n",
      "  (119, 6)\t0.0522302030682\n",
      "  (119, 7)\t0.000195963158926\n",
      "  (120, 0)\t0.000755857898715\n",
      "  (120, 1)\t0.0961228787316\n",
      "  (120, 2)\t0.0439144107574\n",
      "  (120, 3)\t0.00158506909562\n",
      "  (120, 4)\t0.0657834572714\n",
      "  (120, 5)\t0.0302331602026\n",
      "  (120, 6)\t0.0522302030682\n",
      "  (120, 7)\t0.000195963158926\n",
      "  (121, 0)\t0.000755857898715\n",
      "  (121, 1)\t0.0961228787316\n",
      "  (121, 2)\t0.0439144107574\n",
      "  (121, 3)\t0.033286451008\n",
      "  (121, 4)\t0.0316102327148\n",
      "  (121, 5)\t0.00163422487582\n",
      "  (121, 6)\t0.0648665425202\n",
      "  (121, 7)\t0.000195963158926\n",
      "  (122, 0)\t0.000755857898715\n",
      "  (122, 1)\t0.0832492789015\n",
      "  (122, 2)\t0.0819172662206\n",
      "  (122, 3)\t0.00158506909562\n",
      "  (122, 4)\t0.0914133756889\n",
      "  (122, 5)\t0.0588320955293\n",
      "  (122, 6)\t0.0522302030682\n",
      "  (122, 7)\t0.000195963158926\n",
      "  (123, 0)\t0.000755857898715\n",
      "  (123, 1)\t0.0832492789015\n",
      "  (123, 2)\t0.0903623452124\n",
      "  (123, 3)\t0.00158506909562\n",
      "  (123, 4)\t0.129858253315\n",
      "  (123, 5)\t0.0588320955293\n",
      "  (123, 6)\t0.0606544293695\n",
      "  (123, 7)\t0.000195963158926\n",
      "  (124, 0)\t0.000755857898715\n",
      "  (124, 1)\t0.0188812797508\n",
      "  (124, 2)\t0.0819172662206\n",
      "  (124, 3)\t0.033286451008\n",
      "  (124, 4)\t0.0914133756889\n",
      "  (124, 5)\t0.00163422487582\n",
      "  (124, 6)\t0.0522302030682\n",
      "  (124, 7)\t0.000195963158926\n",
      "  (125, 0)\t0.000755857898715\n",
      "  (125, 1)\t0.0703756790713\n",
      "  (125, 2)\t0.0903623452124\n",
      "  (125, 3)\t0.033286451008\n",
      "  (125, 4)\t0.129858253315\n",
      "  (125, 5)\t0.0588320955293\n",
      "  (125, 6)\t0.00168484526026\n",
      "  (125, 7)\t0.000195963158926\n",
      "  (126, 0)\t0.000755857898715\n",
      "  (126, 1)\t0.0703756790713\n",
      "  (126, 2)\t0.0312467922697\n",
      "  (126, 3)\t0.037249123747\n",
      "  (126, 4)\t0.0657834572714\n",
      "  (126, 5)\t0.0588320955293\n",
      "  (126, 6)\t0.0395938636162\n",
      "  (126, 7)\t0.000195963158926\n",
      "  (127, 0)\t0.000755857898715\n",
      "  (127, 1)\t0.0832492789015\n",
      "  (127, 2)\t0.0819172662206\n",
      "  (127, 3)\t0.00158506909562\n",
      "  (127, 4)\t0.129858253315\n",
      "  (127, 5)\t0.0302331602026\n",
      "  (127, 6)\t0.0606544293695\n",
      "  (127, 7)\t0.000195963158926\n",
      "  (128, 0)\t0.000755857898715\n",
      "  (128, 1)\t0.0961228787316\n",
      "  (128, 2)\t0.0439144107574\n",
      "  (128, 3)\t0.00158506909562\n",
      "  (128, 4)\t0.0657834572714\n",
      "  (128, 5)\t0.00980534925489\n",
      "  (128, 6)\t0.0648665425202\n",
      "  (128, 7)\t0.000195963158926\n",
      "  (129, 0)\t0.000755857898715\n",
      "  (129, 1)\t0.117578878448\n",
      "  (129, 2)\t0.0312467922697\n",
      "  (129, 3)\t0.033286451008\n",
      "  (129, 4)\t0.0316102327148\n",
      "  (129, 5)\t0.00980534925489\n",
      "  (129, 6)\t0.0185332978629\n",
      "  (129, 7)\t0.000195963158926\n",
      "  (130, 0)\t0.000755857898715\n",
      "  (130, 1)\t0.104705278618\n",
      "  (130, 2)\t0.0903623452124\n",
      "  (130, 3)\t0.00158506909562\n",
      "  (130, 4)\t0.0316102327148\n",
      "  (130, 5)\t0.00163422487582\n",
      "  (130, 6)\t0.0353817504655\n",
      "  (130, 7)\t0.000195963158926\n",
      "  (131, 0)\t0.00453514739229\n",
      "  (131, 1)\t0.0961228787316\n",
      "  (131, 2)\t0.0439144107574\n",
      "  (131, 3)\t0.033286451008\n",
      "  (131, 4)\t0.0316102327148\n",
      "  (131, 5)\t0.0588320955293\n",
      "  (131, 6)\t0.0395938636162\n",
      "  (131, 7)\t0.000195963158926\n",
      "  (132, 0)\t0.000755857898715\n",
      "  (132, 1)\t0.0832492789015\n",
      "  (132, 2)\t0.0439144107574\n",
      "  (132, 3)\t0.033286451008\n",
      "  (132, 4)\t0.099956681828\n",
      "  (132, 5)\t0.0302331602026\n",
      "  (132, 6)\t0.0522302030682\n",
      "  (132, 7)\t0.000195963158926\n",
      "  (133, 0)\t0.000755857898715\n",
      "  (133, 1)\t0.0832492789015\n",
      "  (133, 2)\t0.0903623452124\n",
      "  (133, 3)\t0.037249123747\n",
      "  (133, 4)\t0.099956681828\n",
      "  (133, 5)\t0.0506609711503\n",
      "  (133, 6)\t0.0185332978629\n",
      "  (133, 7)\t0.000195963158926\n",
      "  (134, 0)\t0.000755857898715\n",
      "  (134, 1)\t0.0961228787316\n",
      "  (134, 2)\t0.0439144107574\n",
      "  (134, 3)\t0.033286451008\n",
      "  (134, 4)\t0.099956681828\n",
      "  (134, 5)\t0.0302331602026\n",
      "  (134, 6)\t0.0606544293695\n",
      "  (134, 7)\t0.000195963158926\n",
      "  (135, 0)\t0.000755857898715\n",
      "  (135, 1)\t0.117578878448\n",
      "  (135, 2)\t0.0903623452124\n",
      "  (135, 3)\t0.00158506909562\n",
      "  (135, 4)\t0.099956681828\n",
      "  (135, 5)\t0.0302331602026\n",
      "  (135, 6)\t0.0606544293695\n",
      "  (135, 7)\t0.000195963158926\n",
      "  (136, 0)\t0.000755857898715\n",
      "  (136, 1)\t0.0832492789015\n",
      "  (136, 2)\t0.0312467922697\n",
      "  (136, 3)\t0.033286451008\n",
      "  (136, 4)\t0.099956681828\n",
      "  (136, 5)\t0.00163422487582\n",
      "  (136, 6)\t0.00168484526026\n",
      "  (136, 7)\t0.000195963158926\n",
      "  (137, 0)\t0.000755857898715\n",
      "  (137, 1)\t0.104705278618\n",
      "  (137, 2)\t0.0439144107574\n",
      "  (137, 3)\t0.033286451008\n",
      "  (137, 4)\t0.099956681828\n",
      "  (137, 5)\t0.0506609711503\n",
      "  (137, 6)\t0.0185332978629\n",
      "  (137, 7)\t0.000195963158926\n",
      "  (138, 0)\t0.000755857898715\n",
      "  (138, 1)\t0.0832492789015\n",
      "  (138, 2)\t0.0819172662206\n",
      "  (138, 3)\t0.00158506909562\n",
      "  (138, 4)\t0.0914133756889\n",
      "  (138, 5)\t0.0588320955293\n",
      "  (138, 6)\t0.0522302030682\n",
      "  (138, 7)\t0.000195963158926\n",
      "  (139, 0)\t0.000755857898715\n",
      "  (139, 1)\t0.0961228787316\n",
      "  (139, 2)\t0.0439144107574\n",
      "  (139, 3)\t0.037249123747\n",
      "  (139, 4)\t0.0316102327148\n",
      "  (139, 5)\t0.0302331602026\n",
      "  (139, 6)\t0.0185332978629\n",
      "  (139, 7)\t0.000195963158926\n",
      "  (140, 0)\t0.000755857898715\n",
      "  (140, 1)\t0.0961228787316\n",
      "  (140, 2)\t0.0903623452124\n",
      "  (140, 3)\t0.0412117964861\n",
      "  (140, 4)\t0.0657834572714\n",
      "  (140, 5)\t0.0302331602026\n",
      "  (140, 6)\t0.0522302030682\n",
      "  (140, 7)\t0.000195963158926\n",
      "  (141, 0)\t0.000755857898715\n",
      "  (141, 1)\t0.104705278618\n",
      "  (141, 2)\t0.0903623452124\n",
      "  (141, 3)\t0.0412117964861\n",
      "  (141, 4)\t0.0316102327148\n",
      "  (141, 5)\t0.0506609711503\n",
      "  (141, 6)\t0.0606544293695\n",
      "  (141, 7)\t0.000195963158926\n",
      "  (142, 0)\t0.000755857898715\n",
      "  (142, 1)\t0.117578878448\n",
      "  (142, 2)\t0.0312467922697\n",
      "  (142, 3)\t0.033286451008\n",
      "  (142, 4)\t0.099956681828\n",
      "  (142, 5)\t0.0506609711503\n",
      "  (142, 6)\t0.00168484526026\n",
      "  (142, 7)\t0.000195963158926\n",
      "  (143, 0)\t0.000755857898715\n",
      "  (143, 1)\t0.117578878448\n",
      "  (143, 2)\t0.0228017132779\n",
      "  (143, 3)\t0.037249123747\n",
      "  (143, 4)\t0.129858253315\n",
      "  (143, 5)\t0.0588320955293\n",
      "  (143, 6)\t0.0606544293695\n",
      "  (143, 7)\t0.000195963158926\n",
      "  (144, 0)\t0.00453514739229\n",
      "  (144, 1)\t0.0832492789015\n",
      "  (144, 2)\t0.00168901579836\n",
      "  (144, 3)\t0.037249123747\n",
      "  (144, 4)\t0.129858253315\n",
      "  (144, 5)\t0.0588320955293\n",
      "  (144, 6)\t0.0522302030682\n",
      "  (144, 7)\t0.000195963158926\n",
      "  (145, 0)\t0.000755857898715\n",
      "  (145, 1)\t0.0961228787316\n",
      "  (145, 2)\t0.0819172662206\n",
      "  (145, 3)\t0.00158506909562\n",
      "  (145, 4)\t0.0657834572714\n",
      "  (145, 5)\t0.0506609711503\n",
      "  (145, 6)\t0.0648665425202\n",
      "  (145, 7)\t0.000195963158926\n",
      "  (146, 0)\t0.000755857898715\n",
      "  (146, 1)\t0.0832492789015\n",
      "  (146, 2)\t0.0819172662206\n",
      "  (146, 3)\t0.033286451008\n",
      "  (146, 4)\t0.0914133756889\n",
      "  (146, 5)\t0.0588320955293\n",
      "  (146, 6)\t0.0606544293695\n",
      "  (146, 7)\t0.000195963158926\n",
      "  (147, 0)\t0.000755857898715\n",
      "  (147, 1)\t0.117578878448\n",
      "  (147, 2)\t0.0312467922697\n",
      "  (147, 3)\t0.00158506909562\n",
      "  (147, 4)\t0.099956681828\n",
      "  (147, 5)\t0.0588320955293\n",
      "  (147, 6)\t0.0185332978629\n",
      "  (147, 7)\t0.000195963158926\n",
      "  (148, 0)\t0.000755857898715\n",
      "  (148, 1)\t0.0961228787316\n",
      "  (148, 2)\t0.0819172662206\n",
      "  (148, 3)\t0.033286451008\n",
      "  (148, 4)\t0.0145236204366\n",
      "  (148, 5)\t0.0302331602026\n",
      "  (148, 6)\t0.0648665425202\n",
      "  (148, 7)\t0.000195963158926\n",
      "  (149, 0)\t0.000755857898715\n",
      "  (149, 1)\t0.0188812797508\n",
      "  (149, 2)\t0.0819172662206\n",
      "  (149, 3)\t0.033286451008\n",
      "  (149, 4)\t0.0914133756889\n",
      "  (149, 5)\t0.0302331602026\n",
      "  (149, 6)\t0.0606544293695\n",
      "  (149, 7)\t0.000195963158926\n",
      "  (150, 0)\t0.000755857898715\n",
      "  (150, 1)\t0.104705278618\n",
      "  (150, 2)\t0.0312467922697\n",
      "  (150, 3)\t0.033286451008\n",
      "  (150, 4)\t0.129858253315\n",
      "  (150, 5)\t0.00163422487582\n",
      "  (150, 6)\t0.0648665425202\n",
      "  (150, 7)\t0.000195963158926\n",
      "  (151, 0)\t0.000755857898715\n",
      "  (151, 1)\t0.00171647997735\n",
      "  (151, 2)\t0.00168901579836\n",
      "  (151, 3)\t0.00158506909562\n",
      "  (151, 4)\t0.00170866122783\n",
      "  (151, 5)\t0.00163422487582\n",
      "  (151, 6)\t0.00168484526026\n",
      "  (151, 7)\t0.000195963158926\n",
      "  (152, 0)\t0.000755857898715\n",
      "  (152, 1)\t0.0832492789015\n",
      "  (152, 2)\t0.0819172662206\n",
      "  (152, 3)\t0.0412117964861\n",
      "  (152, 4)\t0.099956681828\n",
      "  (152, 5)\t0.0302331602026\n",
      "  (152, 6)\t0.0606544293695\n",
      "  (152, 7)\t0.000195963158926\n",
      "  (153, 0)\t0.000755857898715\n",
      "  (153, 1)\t0.0703756790713\n",
      "  (153, 2)\t0.0312467922697\n",
      "  (153, 3)\t0.033286451008\n",
      "  (153, 4)\t0.099956681828\n",
      "  (153, 5)\t0.0302331602026\n",
      "  (153, 6)\t0.0648665425202\n",
      "  (153, 7)\t0.000195963158926\n",
      "  (154, 0)\t0.000755857898715\n",
      "  (154, 1)\t0.0961228787316\n",
      "  (154, 2)\t0.0439144107574\n",
      "  (154, 3)\t0.033286451008\n",
      "  (154, 4)\t0.0316102327148\n",
      "  (154, 5)\t0.00163422487582\n",
      "  (154, 6)\t0.0648665425202\n",
      "  (154, 7)\t0.000195963158926\n",
      "  (155, 0)\t0.000755857898715\n",
      "  (155, 1)\t0.0961228787316\n",
      "  (155, 2)\t0.0819172662206\n",
      "  (155, 3)\t0.00554774183466\n",
      "  (155, 4)\t0.099956681828\n",
      "  (155, 5)\t0.0588320955293\n",
      "  (155, 6)\t0.0522302030682\n",
      "  (155, 7)\t0.000195963158926\n",
      "  (156, 0)\t0.000755857898715\n",
      "  (156, 1)\t0.0832492789015\n",
      "  (156, 2)\t0.0312467922697\n",
      "  (156, 3)\t0.033286451008\n",
      "  (156, 4)\t0.099956681828\n",
      "  (156, 5)\t0.00163422487582\n",
      "  (156, 6)\t0.00168484526026\n",
      "  (156, 7)\t0.000195963158926\n",
      "  (157, 0)\t0.000755857898715\n",
      "  (157, 1)\t0.0188812797508\n",
      "  (157, 2)\t0.0439144107574\n",
      "  (157, 3)\t0.033286451008\n",
      "  (157, 4)\t0.00170866122783\n",
      "  (157, 5)\t0.00163422487582\n",
      "  (157, 6)\t0.0522302030682\n",
      "  (157, 7)\t0.000195963158926\n",
      "  (158, 0)\t0.000755857898715\n",
      "  (158, 1)\t0.0188812797508\n",
      "  (158, 2)\t0.0439144107574\n",
      "  (158, 3)\t0.033286451008\n",
      "  (158, 4)\t0.00170866122783\n",
      "  (158, 5)\t0.00163422487582\n",
      "  (158, 6)\t0.0522302030682\n",
      "  (158, 7)\t0.000195963158926\n",
      "  (159, 0)\t0.000755857898715\n",
      "  (159, 1)\t0.117578878448\n",
      "  (159, 2)\t0.0903623452124\n",
      "  (159, 3)\t0.033286451008\n",
      "  (159, 4)\t0.099956681828\n",
      "  (159, 5)\t0.00163422487582\n",
      "  (159, 6)\t0.0269575241642\n",
      "  (159, 7)\t0.000195963158926\n",
      "  (160, 0)\t0.000755857898715\n",
      "  (160, 1)\t0.0188812797508\n",
      "  (160, 2)\t0.0439144107574\n",
      "  (160, 3)\t0.033286451008\n",
      "  (160, 4)\t0.00170866122783\n",
      "  (160, 5)\t0.00163422487582\n",
      "  (160, 6)\t0.0522302030682\n",
      "  (160, 7)\t0.000195963158926\n",
      "  (161, 0)\t0.000755857898715\n",
      "  (161, 1)\t0.104705278618\n",
      "  (161, 2)\t0.0439144107574\n",
      "  (161, 3)\t0.00158506909562\n",
      "  (161, 4)\t0.0316102327148\n",
      "  (161, 5)\t0.00163422487582\n",
      "  (161, 6)\t0.0353817504655\n",
      "  (161, 7)\t0.000195963158926\n",
      "  (162, 0)\t0.000755857898715\n",
      "  (162, 1)\t0.104705278618\n",
      "  (162, 2)\t0.0819172662206\n",
      "  (162, 3)\t0.033286451008\n",
      "  (162, 4)\t0.129858253315\n",
      "  (162, 5)\t0.0302331602026\n",
      "  (162, 6)\t0.0353817504655\n",
      "  (162, 7)\t0.000195963158926\n",
      "  (163, 0)\t0.000755857898715\n",
      "  (163, 1)\t0.117578878448\n",
      "  (163, 2)\t0.0439144107574\n",
      "  (163, 3)\t0.0412117964861\n",
      "  (163, 4)\t0.0657834572714\n",
      "  (163, 5)\t0.0302331602026\n",
      "  (163, 6)\t0.0522302030682\n",
      "  (163, 7)\t0.000195963158926\n",
      "  (164, 0)\t0.000755857898715\n",
      "  (164, 1)\t0.0703756790713\n",
      "  (164, 2)\t0.0903623452124\n",
      "  (164, 3)\t0.033286451008\n",
      "  (164, 4)\t0.129858253315\n",
      "  (164, 5)\t0.0588320955293\n",
      "  (164, 6)\t0.00168484526026\n",
      "  (164, 7)\t0.000195963158926\n",
      "  (165, 0)\t0.000755857898715\n",
      "  (165, 1)\t0.104705278618\n",
      "  (165, 2)\t0.0903623452124\n",
      "  (165, 3)\t0.0412117964861\n",
      "  (165, 4)\t0.099956681828\n",
      "  (165, 5)\t0.0588320955293\n",
      "  (165, 6)\t0.00168484526026\n",
      "  (165, 7)\t0.000195963158926\n",
      "  (166, 0)\t0.000755857898715\n",
      "  (166, 1)\t0.104705278618\n",
      "  (166, 2)\t0.0312467922697\n",
      "  (166, 3)\t0.037249123747\n",
      "  (166, 4)\t0.0914133756889\n",
      "  (166, 5)\t0.0302331602026\n",
      "  (166, 6)\t0.0648665425202\n",
      "  (166, 7)\t0.000195963158926\n",
      "  (167, 0)\t0.000755857898715\n",
      "  (167, 1)\t0.00171647997735\n",
      "  (167, 2)\t0.00168901579836\n",
      "  (167, 3)\t0.00158506909562\n",
      "  (167, 4)\t0.00170866122783\n",
      "  (167, 5)\t0.00163422487582\n",
      "  (167, 6)\t0.00168484526026\n",
      "  (167, 7)\t0.000195963158926\n",
      "  (168, 0)\t0.000755857898715\n",
      "  (168, 1)\t0.00171647997735\n",
      "  (168, 2)\t0.00168901579836\n",
      "  (168, 3)\t0.00158506909562\n",
      "  (168, 4)\t0.00170866122783\n",
      "  (168, 5)\t0.00163422487582\n",
      "  (168, 6)\t0.00168484526026\n",
      "  (168, 7)\t0.000195963158926\n",
      "  (169, 0)\t0.000755857898715\n",
      "  (169, 1)\t0.104705278618\n",
      "  (169, 2)\t0.0819172662206\n",
      "  (169, 3)\t0.033286451008\n",
      "  (169, 4)\t0.099956681828\n",
      "  (169, 5)\t0.0302331602026\n",
      "  (169, 6)\t0.0606544293695\n",
      "  (169, 7)\t0.000195963158926\n",
      "  (170, 0)\t0.000755857898715\n",
      "  (170, 1)\t0.0188812797508\n",
      "  (170, 2)\t0.0439144107574\n",
      "  (170, 3)\t0.00158506909562\n",
      "  (170, 4)\t0.0657834572714\n",
      "  (170, 5)\t0.0302331602026\n",
      "  (170, 6)\t0.0395938636162\n",
      "  (170, 7)\t0.000195963158926\n",
      "  (171, 0)\t0.000755857898715\n",
      "  (171, 1)\t0.00171647997735\n",
      "  (171, 2)\t0.0312467922697\n",
      "  (171, 3)\t0.00158506909562\n",
      "  (171, 4)\t0.00170866122783\n",
      "  (171, 5)\t0.00163422487582\n",
      "  (171, 6)\t0.0606544293695\n",
      "  (171, 7)\t0.000195963158926\n",
      "  (172, 0)\t0.000755857898715\n",
      "  (172, 1)\t0.0961228787316\n",
      "  (172, 2)\t0.0312467922697\n",
      "  (172, 3)\t0.0412117964861\n",
      "  (172, 4)\t0.129858253315\n",
      "  (172, 5)\t0.0302331602026\n",
      "  (172, 6)\t0.00168484526026\n",
      "  (172, 7)\t0.000195963158926\n",
      "  (173, 0)\t0.00453514739229\n",
      "  (173, 1)\t0.0832492789015\n",
      "  (173, 2)\t0.0312467922697\n",
      "  (173, 3)\t0.0412117964861\n",
      "  (173, 4)\t0.129858253315\n",
      "  (173, 5)\t0.0506609711503\n",
      "  (173, 6)\t0.0185332978629\n",
      "  (173, 7)\t0.000195963158926\n",
      "  (174, 0)\t0.000755857898715\n",
      "  (174, 1)\t0.00171647997735\n",
      "  (174, 2)\t0.00168901579836\n",
      "  (174, 3)\t0.00158506909562\n",
      "  (174, 4)\t0.00170866122783\n",
      "  (174, 5)\t0.00163422487582\n",
      "  (174, 6)\t0.00168484526026\n",
      "  (174, 7)\t0.000195963158926\n",
      "  (175, 0)\t0.000755857898715\n",
      "  (175, 1)\t0.104705278618\n",
      "  (175, 2)\t0.0903623452124\n",
      "  (175, 3)\t0.00158506909562\n",
      "  (175, 4)\t0.0657834572714\n",
      "  (175, 5)\t0.0588320955293\n",
      "  (175, 6)\t0.0606544293695\n",
      "  (175, 7)\t0.000195963158926\n",
      "  (176, 0)\t0.000755857898715\n",
      "  (176, 1)\t0.0832492789015\n",
      "  (176, 2)\t0.0439144107574\n",
      "  (176, 3)\t0.037249123747\n",
      "  (176, 4)\t0.129858253315\n",
      "  (176, 5)\t0.0588320955293\n",
      "  (176, 6)\t0.00168484526026\n",
      "  (176, 7)\t0.000195963158926\n",
      "  (177, 0)\t0.000755857898715\n",
      "  (177, 1)\t0.0961228787316\n",
      "  (177, 2)\t0.0439144107574\n",
      "  (177, 3)\t0.00158506909562\n",
      "  (177, 4)\t0.129858253315\n",
      "  (177, 5)\t0.0506609711503\n",
      "  (177, 6)\t0.0185332978629\n",
      "  (177, 7)\t0.000195963158926\n",
      "  (178, 0)\t0.000755857898715\n",
      "  (178, 1)\t0.117578878448\n",
      "  (178, 2)\t0.0439144107574\n",
      "  (178, 3)\t0.00158506909562\n",
      "  (178, 4)\t0.099956681828\n",
      "  (178, 5)\t0.00163422487582\n",
      "  (178, 6)\t0.0606544293695\n",
      "  (178, 7)\t0.000195963158926\n",
      "  (179, 0)\t0.000755857898715\n",
      "  (179, 1)\t0.0703756790713\n",
      "  (179, 2)\t0.0439144107574\n",
      "  (179, 3)\t0.037249123747\n",
      "  (179, 4)\t0.0914133756889\n",
      "  (179, 5)\t0.0588320955293\n",
      "  (179, 6)\t0.0395938636162\n",
      "  (179, 7)\t0.000195963158926\n",
      "  (180, 0)\t0.00453514739229\n",
      "  (180, 1)\t0.0961228787316\n",
      "  (180, 2)\t0.0312467922697\n",
      "  (180, 3)\t0.033286451008\n",
      "  (180, 4)\t0.099956681828\n",
      "  (180, 5)\t0.0588320955293\n",
      "  (180, 6)\t0.0395938636162\n",
      "  (180, 7)\t0.000195963158926\n",
      "  (181, 0)\t0.000755857898715\n",
      "  (181, 1)\t0.117578878448\n",
      "  (181, 2)\t0.0903623452124\n",
      "  (181, 3)\t0.033286451008\n",
      "  (181, 4)\t0.099956681828\n",
      "  (181, 5)\t0.00163422487582\n",
      "  (181, 6)\t0.0269575241642\n",
      "  (181, 7)\t0.000195963158926\n",
      "  (182, 0)\t0.000755857898715\n",
      "  (182, 1)\t0.0961228787316\n",
      "  (182, 2)\t0.0819172662206\n",
      "  (182, 3)\t0.037249123747\n",
      "  (182, 4)\t0.0145236204366\n",
      "  (182, 5)\t0.0302331602026\n",
      "  (182, 6)\t0.0522302030682\n",
      "  (182, 7)\t0.000195963158926\n",
      "  (183, 0)\t0.000755857898715\n",
      "  (183, 1)\t0.0703756790713\n",
      "  (183, 2)\t0.0312467922697\n",
      "  (183, 3)\t0.037249123747\n",
      "  (183, 4)\t0.0657834572714\n",
      "  (183, 5)\t0.0588320955293\n",
      "  (183, 6)\t0.0395938636162\n",
      "  (183, 7)\t0.000195963158926\n",
      "  (184, 0)\t0.000755857898715\n",
      "  (184, 1)\t0.0832492789015\n",
      "  (184, 2)\t0.0819172662206\n",
      "  (184, 3)\t0.037249123747\n",
      "  (184, 4)\t0.0914133756889\n",
      "  (184, 5)\t0.0302331602026\n",
      "  (184, 6)\t0.0522302030682\n",
      "  (184, 7)\t0.000195963158926\n",
      "  (185, 0)\t0.000755857898715\n",
      "  (185, 1)\t0.0832492789015\n",
      "  (185, 2)\t0.0819172662206\n",
      "  (185, 3)\t0.00158506909562\n",
      "  (185, 4)\t0.0914133756889\n",
      "  (185, 5)\t0.0588320955293\n",
      "  (185, 6)\t0.0522302030682\n",
      "  (185, 7)\t0.000195963158926\n",
      "  (186, 0)\t0.000755857898715\n",
      "  (186, 1)\t0.117578878448\n",
      "  (186, 2)\t0.0903623452124\n",
      "  (186, 3)\t0.033286451008\n",
      "  (186, 4)\t0.099956681828\n",
      "  (186, 5)\t0.00163422487582\n",
      "  (186, 6)\t0.0269575241642\n",
      "  (186, 7)\t0.000195963158926\n",
      "  (187, 0)\t0.000755857898715\n",
      "  (187, 1)\t0.00171647997735\n",
      "  (187, 2)\t0.00168901579836\n",
      "  (187, 3)\t0.00158506909562\n",
      "  (187, 4)\t0.00170866122783\n",
      "  (187, 5)\t0.00163422487582\n",
      "  (187, 6)\t0.00168484526026\n",
      "  (187, 7)\t0.000195963158926\n",
      "  (188, 0)\t0.000755857898715\n",
      "  (188, 1)\t0.0961228787316\n",
      "  (188, 2)\t0.0439144107574\n",
      "  (188, 3)\t0.037249123747\n",
      "  (188, 4)\t0.099956681828\n",
      "  (188, 5)\t0.0506609711503\n",
      "  (188, 6)\t0.0185332978629\n",
      "  (188, 7)\t0.000195963158926\n",
      "  (189, 0)\t0.000755857898715\n",
      "  (189, 1)\t0.0961228787316\n",
      "  (189, 2)\t0.0819172662206\n",
      "  (189, 3)\t0.00158506909562\n",
      "  (189, 4)\t0.099956681828\n",
      "  (189, 5)\t0.0588320955293\n",
      "  (189, 6)\t0.0522302030682\n",
      "  (189, 7)\t0.000195963158926\n",
      "  (190, 0)\t0.000755857898715\n",
      "  (190, 1)\t0.117578878448\n",
      "  (190, 2)\t0.00168901579836\n",
      "  (190, 3)\t0.00158506909562\n",
      "  (190, 4)\t0.099956681828\n",
      "  (190, 5)\t0.00163422487582\n",
      "  (190, 6)\t0.0606544293695\n",
      "  (190, 7)\t0.000195963158926\n",
      "  (191, 0)\t0.00453514739229\n",
      "  (191, 1)\t0.0832492789015\n",
      "  (191, 2)\t0.00168901579836\n",
      "  (191, 3)\t0.037249123747\n",
      "  (191, 4)\t0.0316102327148\n",
      "  (191, 5)\t0.0588320955293\n",
      "  (191, 6)\t0.0522302030682\n",
      "  (191, 7)\t0.000195963158926\n",
      "  (192, 0)\t0.000755857898715\n",
      "  (192, 1)\t0.104705278618\n",
      "  (192, 2)\t0.0903623452124\n",
      "  (192, 3)\t0.00158506909562\n",
      "  (192, 4)\t0.0657834572714\n",
      "  (192, 5)\t0.0138909114444\n",
      "  (192, 6)\t0.0648665425202\n",
      "  (192, 7)\t0.000195963158926\n",
      "  (193, 0)\t0.000755857898715\n",
      "  (193, 1)\t0.0961228787316\n",
      "  (193, 2)\t0.0312467922697\n",
      "  (193, 3)\t0.00158506909562\n",
      "  (193, 4)\t0.099956681828\n",
      "  (193, 5)\t0.00163422487582\n",
      "  (193, 6)\t0.0606544293695\n",
      "  (193, 7)\t0.000195963158926\n",
      "  (194, 0)\t0.000755857898715\n",
      "  (194, 1)\t0.0188812797508\n",
      "  (194, 2)\t0.0439144107574\n",
      "  (194, 3)\t0.033286451008\n",
      "  (194, 4)\t0.00170866122783\n",
      "  (194, 5)\t0.00163422487582\n",
      "  (194, 6)\t0.0522302030682\n",
      "  (194, 7)\t0.000195963158926\n",
      "  (195, 0)\t0.000755857898715\n",
      "  (195, 1)\t0.0832492789015\n",
      "  (195, 2)\t0.0903623452124\n",
      "  (195, 3)\t0.037249123747\n",
      "  (195, 4)\t0.099956681828\n",
      "  (195, 5)\t0.0506609711503\n",
      "  (195, 6)\t0.0185332978629\n",
      "  (195, 7)\t0.000195963158926\n",
      "  (196, 0)\t0.000755857898715\n",
      "  (196, 1)\t0.104705278618\n",
      "  (196, 2)\t0.0903623452124\n",
      "  (196, 3)\t0.0412117964861\n",
      "  (196, 4)\t0.099956681828\n",
      "  (196, 5)\t0.0588320955293\n",
      "  (196, 6)\t0.00168484526026\n",
      "  (196, 7)\t0.000195963158926\n",
      "  (197, 0)\t0.00453514739229\n",
      "  (197, 1)\t0.0961228787316\n",
      "  (197, 2)\t0.0312467922697\n",
      "  (197, 3)\t0.00554774183466\n",
      "  (197, 4)\t0.0914133756889\n",
      "  (197, 5)\t0.0506609711503\n",
      "  (197, 6)\t0.0185332978629\n",
      "  (197, 7)\t0.000195963158926\n",
      "  (198, 0)\t0.000755857898715\n",
      "  (198, 1)\t0.104705278618\n",
      "  (198, 2)\t0.0439144107574\n",
      "  (198, 3)\t0.00158506909562\n",
      "  (198, 4)\t0.099956681828\n",
      "  (198, 5)\t0.00163422487582\n",
      "  (198, 6)\t0.0185332978629\n",
      "  (198, 7)\t0.000195963158926\n",
      "  (199, 0)\t0.000755857898715\n",
      "  (199, 1)\t0.0188812797508\n",
      "  (199, 2)\t0.0439144107574\n",
      "  (199, 3)\t0.037249123747\n",
      "  (199, 4)\t0.0316102327148\n",
      "  (199, 5)\t0.00163422487582\n",
      "  (199, 6)\t0.00168484526026\n",
      "  (199, 7)\t0.000195963158926\n",
      "  (200, 0)\t0.000755857898715\n",
      "  (200, 1)\t0.104705278618\n",
      "  (200, 2)\t0.0439144107574\n",
      "  (200, 3)\t0.00158506909562\n",
      "  (200, 4)\t0.129858253315\n",
      "  (200, 5)\t0.0302331602026\n",
      "  (200, 6)\t0.0606544293695\n",
      "  (200, 7)\t0.000195963158926\n",
      "  (201, 0)\t0.000755857898715\n",
      "  (201, 1)\t0.117578878448\n",
      "  (201, 2)\t0.0312467922697\n",
      "  (201, 3)\t0.033286451008\n",
      "  (201, 4)\t0.099956681828\n",
      "  (201, 5)\t0.0506609711503\n",
      "  (201, 6)\t0.00168484526026\n",
      "  (201, 7)\t0.000195963158926\n",
      "  (202, 0)\t0.000755857898715\n",
      "  (202, 1)\t0.00171647997735\n",
      "  (202, 2)\t0.00168901579836\n",
      "  (202, 3)\t0.00158506909562\n",
      "  (202, 4)\t0.00170866122783\n",
      "  (202, 5)\t0.00163422487582\n",
      "  (202, 6)\t0.00168484526026\n",
      "  (202, 7)\t0.000195963158926\n",
      "  (203, 0)\t0.000755857898715\n",
      "  (203, 1)\t0.104705278618\n",
      "  (203, 2)\t0.0312467922697\n",
      "  (203, 3)\t0.033286451008\n",
      "  (203, 4)\t0.0145236204366\n",
      "  (203, 5)\t0.00980534925489\n",
      "  (203, 6)\t0.0522302030682\n",
      "  (203, 7)\t0.000195963158926\n",
      "  (204, 0)\t0.000755857898715\n",
      "  (204, 1)\t0.0961228787316\n",
      "  (204, 2)\t0.0439144107574\n",
      "  (204, 3)\t0.037249123747\n",
      "  (204, 4)\t0.099956681828\n",
      "  (204, 5)\t0.0506609711503\n",
      "  (204, 6)\t0.0185332978629\n",
      "  (204, 7)\t0.000195963158926\n",
      "  (205, 0)\t0.000755857898715\n",
      "  (205, 1)\t0.0961228787316\n",
      "  (205, 2)\t0.0903623452124\n",
      "  (205, 3)\t0.00554774183466\n",
      "  (205, 4)\t0.129858253315\n",
      "  (205, 5)\t0.00980534925489\n",
      "  (205, 6)\t0.0185332978629\n",
      "  (205, 7)\t0.000195963158926\n",
      "  (206, 0)\t0.000755857898715\n",
      "  (206, 1)\t0.00171647997735\n",
      "  (206, 2)\t0.00168901579836\n",
      "  (206, 3)\t0.00158506909562\n",
      "  (206, 4)\t0.00170866122783\n",
      "  (206, 5)\t0.00163422487582\n",
      "  (206, 6)\t0.00168484526026\n",
      "  (206, 7)\t0.000195963158926\n",
      "  (207, 0)\t0.000755857898715\n",
      "  (207, 1)\t0.0832492789015\n",
      "  (207, 2)\t0.0903623452124\n",
      "  (207, 3)\t0.00158506909562\n",
      "  (207, 4)\t0.129858253315\n",
      "  (207, 5)\t0.00163422487582\n",
      "  (207, 6)\t0.0648665425202\n",
      "  (207, 7)\t0.000195963158926\n",
      "  (208, 0)\t0.000755857898715\n",
      "  (208, 1)\t0.00171647997735\n",
      "  (208, 2)\t0.00168901579836\n",
      "  (208, 3)\t0.00158506909562\n",
      "  (208, 4)\t0.00170866122783\n",
      "  (208, 5)\t0.00163422487582\n",
      "  (208, 6)\t0.00168484526026\n",
      "  (208, 7)\t0.000195963158926\n",
      "  (209, 0)\t0.000755857898715\n",
      "  (209, 1)\t0.00171647997735\n",
      "  (209, 2)\t0.00168901579836\n",
      "  (209, 3)\t0.00158506909562\n",
      "  (209, 4)\t0.00170866122783\n",
      "  (209, 5)\t0.00163422487582\n",
      "  (209, 6)\t0.00168484526026\n",
      "  (209, 7)\t0.000195963158926\n",
      "  (210, 0)\t0.000755857898715\n",
      "  (210, 1)\t0.104705278618\n",
      "  (210, 2)\t0.0903623452124\n",
      "  (210, 3)\t0.0412117964861\n",
      "  (210, 4)\t0.0316102327148\n",
      "  (210, 5)\t0.0506609711503\n",
      "  (210, 6)\t0.0606544293695\n",
      "  (210, 7)\t0.000195963158926\n",
      "  (211, 0)\t0.000755857898715\n",
      "  (211, 1)\t0.0832492789015\n",
      "  (211, 2)\t0.0439144107574\n",
      "  (211, 3)\t0.037249123747\n",
      "  (211, 4)\t0.129858253315\n",
      "  (211, 5)\t0.0588320955293\n",
      "  (211, 6)\t0.00168484526026\n",
      "  (211, 7)\t0.000195963158926\n",
      "  (212, 0)\t0.000755857898715\n",
      "  (212, 1)\t0.117578878448\n",
      "  (212, 2)\t0.0312467922697\n",
      "  (212, 3)\t0.00158506909562\n",
      "  (212, 4)\t0.0914133756889\n",
      "  (212, 5)\t0.00980534925489\n",
      "  (212, 6)\t0.0648665425202\n",
      "  (212, 7)\t0.000195963158926\n",
      "  (213, 0)\t0.000755857898715\n",
      "  (213, 1)\t0.0188812797508\n",
      "  (213, 2)\t0.0819172662206\n",
      "  (213, 3)\t0.033286451008\n",
      "  (213, 4)\t0.0914133756889\n",
      "  (213, 5)\t0.0302331602026\n",
      "  (213, 6)\t0.0606544293695\n",
      "  (213, 7)\t0.000195963158926\n",
      "  (214, 0)\t0.00453514739229\n",
      "  (214, 1)\t0.0961228787316\n",
      "  (214, 2)\t0.0439144107574\n",
      "  (214, 3)\t0.033286451008\n",
      "  (214, 4)\t0.129858253315\n",
      "  (214, 5)\t0.0588320955293\n",
      "  (214, 6)\t0.0353817504655\n",
      "  (214, 7)\t0.000195963158926\n",
      "  (215, 0)\t0.00453514739229\n",
      "  (215, 1)\t0.0188812797508\n",
      "  (215, 2)\t0.0312467922697\n",
      "  (215, 3)\t0.037249123747\n",
      "  (215, 4)\t0.129858253315\n",
      "  (215, 5)\t0.0588320955293\n",
      "  (215, 6)\t0.0185332978629\n",
      "  (215, 7)\t0.000195963158926\n",
      "  (216, 0)\t0.000755857898715\n",
      "  (216, 1)\t0.00171647997735\n",
      "  (216, 2)\t0.00168901579836\n",
      "  (216, 3)\t0.00158506909562\n",
      "  (216, 4)\t0.00170866122783\n",
      "  (216, 5)\t0.00163422487582\n",
      "  (216, 6)\t0.00168484526026\n",
      "  (216, 7)\t0.000195963158926\n",
      "  (217, 0)\t0.000755857898715\n",
      "  (217, 1)\t0.117578878448\n",
      "  (217, 2)\t0.0903623452124\n",
      "  (217, 3)\t0.033286451008\n",
      "  (217, 4)\t0.099956681828\n",
      "  (217, 5)\t0.00163422487582\n",
      "  (217, 6)\t0.0269575241642\n",
      "  (217, 7)\t0.000195963158926\n",
      "  (218, 0)\t0.000755857898715\n",
      "  (218, 1)\t0.0703756790713\n",
      "  (218, 2)\t0.0819172662206\n",
      "  (218, 3)\t0.033286451008\n",
      "  (218, 4)\t0.129858253315\n",
      "  (218, 5)\t0.0302331602026\n",
      "  (218, 6)\t0.0522302030682\n",
      "  (218, 7)\t0.000195963158926\n",
      "  (219, 0)\t0.000755857898715\n",
      "  (219, 1)\t0.117578878448\n",
      "  (219, 2)\t0.0312467922697\n",
      "  (219, 3)\t0.033286451008\n",
      "  (219, 4)\t0.099956681828\n",
      "  (219, 5)\t0.0506609711503\n",
      "  (219, 6)\t0.00168484526026\n",
      "  (219, 7)\t0.000195963158926\n",
      "  (220, 0)\t0.000755857898715\n",
      "  (220, 1)\t0.0961228787316\n",
      "  (220, 2)\t0.0819172662206\n",
      "  (220, 3)\t0.00158506909562\n",
      "  (220, 4)\t0.129858253315\n",
      "  (220, 5)\t0.0588320955293\n",
      "  (220, 6)\t0.0185332978629\n",
      "  (220, 7)\t0.000195963158926\n",
      "  (221, 0)\t0.000755857898715\n",
      "  (221, 1)\t0.0832492789015\n",
      "  (221, 2)\t0.0439144107574\n",
      "  (221, 3)\t0.033286451008\n",
      "  (221, 4)\t0.099956681828\n",
      "  (221, 5)\t0.00163422487582\n",
      "  (221, 6)\t0.00168484526026\n",
      "  (221, 7)\t0.000195963158926\n",
      "  (222, 0)\t0.000755857898715\n",
      "  (222, 1)\t0.104705278618\n",
      "  (222, 2)\t0.0228017132779\n",
      "  (222, 3)\t0.033286451008\n",
      "  (222, 4)\t0.129858253315\n",
      "  (222, 5)\t0.0302331602026\n",
      "  (222, 6)\t0.0185332978629\n",
      "  (222, 7)\t0.000195963158926\n",
      "  (223, 0)\t0.000755857898715\n",
      "  (223, 1)\t0.104705278618\n",
      "  (223, 2)\t0.0819172662206\n",
      "  (223, 3)\t0.00158506909562\n",
      "  (223, 4)\t0.099956681828\n",
      "  (223, 5)\t0.0588320955293\n",
      "  (223, 6)\t0.0648665425202\n",
      "  (223, 7)\t0.000195963158926\n",
      "  (224, 0)\t0.000755857898715\n",
      "  (224, 1)\t0.104705278618\n",
      "  (224, 2)\t0.0439144107574\n",
      "  (224, 3)\t0.033286451008\n",
      "  (224, 4)\t0.129858253315\n",
      "  (224, 5)\t0.0302331602026\n",
      "  (224, 6)\t0.0606544293695\n",
      "  (224, 7)\t0.000195963158926\n",
      "  (225, 0)\t0.000755857898715\n",
      "  (225, 1)\t0.0832492789015\n",
      "  (225, 2)\t0.0819172662206\n",
      "  (225, 3)\t0.00158506909562\n",
      "  (225, 4)\t0.0914133756889\n",
      "  (225, 5)\t0.0588320955293\n",
      "  (225, 6)\t0.0522302030682\n",
      "  (225, 7)\t0.000195963158926\n"
     ]
    }
   ],
   "source": [
    "print (knn.predict_proba(y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
