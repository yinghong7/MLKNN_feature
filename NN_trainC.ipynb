{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import h5py\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential, Model, model_from_json\n",
    "from keras.layers import Dense, Activation\n",
    "from keras import optimizers, layers, callbacks\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import hamming_loss, coverage_error\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "whole = pd.read_csv ('C:/Users/z5023853/OneDrive - UNSW/Jupyter/Feature_selected/ac.csv')\n",
    "train, test = train_test_split (whole,test_size=0.2)\n",
    "train.to_csv (path_or_buf = 'C:/Users/z5023853/OneDrive - UNSW/Jupyter/Feature_selected/train_cat_ac.csv')\n",
    "test.to_csv (path_or_buf = 'C:/Users/z5023853/OneDrive - UNSW/Jupyter/Feature_selected/test_cat_ac.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training cost (IM1, IM2, IM3, OS1), (11,12,13,7)\n",
    "# ivis=46, iev=47, ilc=50, iqt=51, icost=52, icd=54, ipro=55, iothers=57\n",
    "# CS=58, BT=59, per=60, PT=61, Location=62, yr=63, contract=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train set\n",
    "x_o_train = np.genfromtxt ('C:/Users/z5023853/OneDrive - UNSW/Jupyter/Feature_selected/train_cat_ac.csv', \n",
    "                    delimiter = ',', usecols = (11,12,13,7),skip_header =2)\n",
    "y_o_train = np.genfromtxt ('C:/Users/z5023853/OneDrive - UNSW/Jupyter/Feature_selected/train_cat_ac.csv', \n",
    "                    delimiter = ',', usecols = (46,47,50,51,52,54,55,57),skip_header =2)\n",
    "CS_train = pd.read_csv ('C:/Users/z5023853/OneDrive - UNSW/Jupyter/Feature_selected/train_cat_ac.csv', \n",
    "                    names = [\"CS_cat\"], header = 1)\n",
    "BT_train = pd.read_csv ('C:/Users/z5023853/OneDrive - UNSW/Jupyter/Feature_selected/train_cat_ac.csv', \n",
    "                    names = [\"BT_cat\"], header = 1)\n",
    "per_train = pd.read_csv ('C:/Users/z5023853/OneDrive - UNSW/Jupyter/Feature_selected/train_cat_ac.csv', \n",
    "                    names = [\"per_cat\"], header = 1)\n",
    "PT_train = pd.read_csv ('C:/Users/z5023853/OneDrive - UNSW/Jupyter/Feature_selected/train_cat_ac.csv', \n",
    "                    names = [\"PT_cat\"], header = 1)\n",
    "L_train = pd.read_csv ('C:/Users/z5023853/OneDrive - UNSW/Jupyter/Feature_selected/train_cat_ac.csv', \n",
    "                    names = [\"Location\"], header = 1)\n",
    "yr_train = pd.read_csv ('C:/Users/z5023853/OneDrive - UNSW/Jupyter/Feature_selected/train_cat_ac.csv', \n",
    "                    names = [\"yr_cat\"], header = 1)\n",
    "contract_train = pd.read_csv ('C:/Users/z5023853/OneDrive - UNSW/Jupyter/Feature_selected/train_cat_ac.csv', \n",
    "                    names = [\"contract\"], header = 1)\n",
    "#CS_train = CS_train [2:,:]\n",
    "#BT_train = BT_train [2:,:]\n",
    "#per_train = per_train [2:,:]\n",
    "#PT_train = PT_train [2:,:]\n",
    "#L_train = L_train [2:,:]\n",
    "#yr_train = yr_train [2:,:]\n",
    "#contract_train = contract_train [2:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(82, 4)\n",
      "(82, 1)\n"
     ]
    }
   ],
   "source": [
    "#test set\n",
    "x_o_test = np.genfromtxt ('C:/Users/z5023853/OneDrive - UNSW/Jupyter/Feature_selected/test_cat_ac.csv', \n",
    "                    delimiter = ',', usecols = (11,12,13,7),skip_header =1)\n",
    "y_o_test = np.genfromtxt ('C:/Users/z5023853/OneDrive - UNSW/Jupyter/Feature_selected/test_cat_ac.csv', \n",
    "                    delimiter = ',', usecols = (46,47,50,51,52,54,55,57),skip_header =1)\n",
    "CS_test = pd.read_csv ('C:/Users/z5023853/OneDrive - UNSW/Jupyter/Feature_selected/test_cat_ac.csv', \n",
    "                    names = [\"CS_cat\"], header =0)\n",
    "BT_test = pd.read_csv ('C:/Users/z5023853/OneDrive - UNSW/Jupyter/Feature_selected/test_cat_ac.csv', \n",
    "                    names = [\"BT_cat\"], header =0)\n",
    "per_test = pd.read_csv ('C:/Users/z5023853/OneDrive - UNSW/Jupyter/Feature_selected/test_cat_ac.csv', \n",
    "                    names = [\"per_cat\"], header =0)\n",
    "PT_test = pd.read_csv ('C:/Users/z5023853/OneDrive - UNSW/Jupyter/Feature_selected/test_cat_ac.csv', \n",
    "                    names = [\"PT_cat\"], header = 0)\n",
    "L_test = pd.read_csv ('C:/Users/z5023853/OneDrive - UNSW/Jupyter/Feature_selected/test_cat_ac.csv', \n",
    "                    names = [\"Location\"], header =0)\n",
    "yr_test = pd.read_csv ('C:/Users/z5023853/OneDrive - UNSW/Jupyter/Feature_selected/test_cat_ac.csv', \n",
    "                    names = [\"yr_cat\"], header =0)\n",
    "contract_test = pd.read_csv ('C:/Users/z5023853/OneDrive - UNSW/Jupyter/Feature_selected/test_cat_ac.csv', \n",
    "                    names = [\"contract\"], header =0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = LabelBinarizer (neg_label=-1, pos_label=1)\n",
    "#CS\n",
    "lb.fit (CS_train)\n",
    "CS_train = lb.transform (CS_train)\n",
    "CS_test = lb.transform (CS_test)\n",
    "\n",
    "#BT\n",
    "lb.fit (BT_train)\n",
    "BT_train = lb.transform (BT_train)\n",
    "BT_test = lb.transform (BT_test)\n",
    "\n",
    "#per\n",
    "lb.fit (per_train)\n",
    "per_train = lb.transform (per_train)\n",
    "per_test = lb.transform (per_test)\n",
    "\n",
    "\n",
    "#PT\n",
    "lb.fit (PT_train)\n",
    "PT_train = lb.transform (PT_train)\n",
    "PT_test = lb.transform (PT_test)\n",
    "\n",
    "\n",
    "#Location\n",
    "lb.fit (L_train)\n",
    "L_train = lb.transform (L_train)\n",
    "L_test = lb.transform (L_test)\n",
    "\n",
    "\n",
    "#yr\n",
    "lb.fit (yr_train)\n",
    "yr_train = lb.transform (yr_train)\n",
    "yr_test = lb.transform (yr_test)\n",
    "\n",
    "\n",
    "#contract\n",
    "lb.fit (contract_train)\n",
    "contract_train = lb.transform (contract_train)\n",
    "contract_test = lb.transform (contract_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler ()\n",
    "scaler.fit (x_o_train)\n",
    "x_o_train = scaler.transform (x_o_train)\n",
    "x_o_test = scaler.transform (x_o_test)\n",
    "\n",
    "x_train_split = np.hstack ((x_o_train, CS_train, BT_train, per_train, PT_train, L_train, yr_train, contract_train))\n",
    "x_test = np.hstack ((x_o_test, CS_test, BT_test, per_test, PT_test, L_test, yr_test, contract_test))\n",
    "\n",
    "x_train, x_cv, y_train, y_cv = train_test_split (x_train_split,y_o_train,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(226, 11)\n",
      "(226, 8)\n",
      "(97, 11)\n"
     ]
    }
   ],
   "source": [
    "print (x_train.shape)\n",
    "print (y_train.shape)\n",
    "print (x_cv.shape)\n",
    "#print (CS_train.shape)\n",
    "#print (CS_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 10)                120       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 8)                 88        \n",
      "=================================================================\n",
      "Total params: 758\n",
      "Trainable params: 758\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 226 samples, validate on 97 samples\n",
      "Epoch 1/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.8766 - binary_accuracy: 0.6670 - val_loss: 2.7054 - val_binary_accuracy: 0.6843\n",
      "Epoch 2/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.7659 - binary_accuracy: 0.6764 - val_loss: 2.5786 - val_binary_accuracy: 0.6778\n",
      "Epoch 3/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.6142 - binary_accuracy: 0.6715 - val_loss: 2.4478 - val_binary_accuracy: 0.6611\n",
      "Epoch 4/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.4855 - binary_accuracy: 0.6698 - val_loss: 2.4220 - val_binary_accuracy: 0.6559\n",
      "Epoch 5/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.4055 - binary_accuracy: 0.6626 - val_loss: 2.3379 - val_binary_accuracy: 0.6302\n",
      "Epoch 6/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.3699 - binary_accuracy: 0.6477 - val_loss: 2.3037 - val_binary_accuracy: 0.6289\n",
      "Epoch 7/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.2920 - binary_accuracy: 0.6449 - val_loss: 2.2433 - val_binary_accuracy: 0.6314\n",
      "Epoch 8/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.2292 - binary_accuracy: 0.6405 - val_loss: 2.1759 - val_binary_accuracy: 0.6314\n",
      "Epoch 9/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.1849 - binary_accuracy: 0.6394 - val_loss: 2.1502 - val_binary_accuracy: 0.6314\n",
      "Epoch 10/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.1375 - binary_accuracy: 0.6410 - val_loss: 2.1154 - val_binary_accuracy: 0.6314\n",
      "<keras.callbacks.History object at 0x000000000E22D198>\n",
      "82/82 [==============================] - ETA: 0s\n",
      "[1.9231941554604508, 0.63109754498411974]\n",
      "[ 5.55424409] [-7.81876095] [-3.76212147] [ 5.09534374] [ 2.88647476] [ 6.38440818] [ 11.81466716] [-1.84750006] [-0.15214071]\n",
      "hamming_loss_train 0.3877212389380531\n",
      "hamming_loss_cv 0.4072164948453608\n",
      "hamming_loss_test 0.3826219512195122\n",
      "hamming_loss_traindataset 0.39357585139318885\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_22 (Dense)             (None, 11)                132       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 11)                0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 11)                132       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 11)                0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 11)                132       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 11)                0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 11)                132       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 11)                0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 11)                132       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 11)                0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 11)                132       \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 8)                 96        \n",
      "=================================================================\n",
      "Total params: 888\n",
      "Trainable params: 888\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 226 samples, validate on 97 samples\n",
      "Epoch 1/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 3.7161 - binary_accuracy: 0.6079 - val_loss: 3.4223 - val_binary_accuracy: 0.6198\n",
      "Epoch 2/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 3.4212 - binary_accuracy: 0.6101 - val_loss: 3.2770 - val_binary_accuracy: 0.6211\n",
      "Epoch 3/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 3.2343 - binary_accuracy: 0.6150 - val_loss: 3.0988 - val_binary_accuracy: 0.6302\n",
      "Epoch 4/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 3.0251 - binary_accuracy: 0.6206 - val_loss: 2.8961 - val_binary_accuracy: 0.6340\n",
      "Epoch 5/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.7790 - binary_accuracy: 0.6267 - val_loss: 2.6526 - val_binary_accuracy: 0.6405\n",
      "Epoch 6/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.6075 - binary_accuracy: 0.6311 - val_loss: 2.5647 - val_binary_accuracy: 0.6405\n",
      "Epoch 7/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.4416 - binary_accuracy: 0.6338 - val_loss: 2.5102 - val_binary_accuracy: 0.6379\n",
      "Epoch 8/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.2717 - binary_accuracy: 0.6399 - val_loss: 2.2480 - val_binary_accuracy: 0.6456\n",
      "Epoch 9/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.0630 - binary_accuracy: 0.6527 - val_loss: 2.0066 - val_binary_accuracy: 0.6598\n",
      "Epoch 10/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.8687 - binary_accuracy: 0.6593 - val_loss: 1.7239 - val_binary_accuracy: 0.6559\n",
      "<keras.callbacks.History object at 0x0000000012364E48>\n",
      "82/82 [==============================] - ETA: 0s\n",
      "[1.4284648400981252, 0.66615855403062774]\n",
      "[-0.14870879] [-0.37905574] [-0.70040693] [ 2.81846528] [-4.7896065] [-8.59473461] [-4.1893131] [-7.43414246] [ 0.8462098]\n",
      "hamming_loss_train 0.3915929203539823\n",
      "hamming_loss_cv 0.4007731958762887\n",
      "hamming_loss_test 0.3719512195121951\n",
      "hamming_loss_traindataset 0.3943498452012384\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_29 (Dense)             (None, 12)                144       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 8)                 104       \n",
      "=================================================================\n",
      "Total params: 1,028\n",
      "Trainable params: 1,028\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 226 samples, validate on 97 samples\n",
      "Epoch 1/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 3.4536 - binary_accuracy: 0.6405 - val_loss: 3.3606 - val_binary_accuracy: 0.6959\n",
      "Epoch 2/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 3.2150 - binary_accuracy: 0.6936 - val_loss: 2.9553 - val_binary_accuracy: 0.7010\n",
      "Epoch 3/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.8293 - binary_accuracy: 0.6952 - val_loss: 2.6641 - val_binary_accuracy: 0.6985\n",
      "Epoch 4/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.6702 - binary_accuracy: 0.6869 - val_loss: 2.5567 - val_binary_accuracy: 0.6778\n",
      "Epoch 5/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.4985 - binary_accuracy: 0.6692 - val_loss: 2.3805 - val_binary_accuracy: 0.6508\n",
      "Epoch 6/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.1857 - binary_accuracy: 0.6554 - val_loss: 1.8109 - val_binary_accuracy: 0.6340\n",
      "Epoch 7/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.6929 - binary_accuracy: 0.6327 - val_loss: 1.5866 - val_binary_accuracy: 0.6276\n",
      "Epoch 8/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.5221 - binary_accuracy: 0.6289 - val_loss: 1.5617 - val_binary_accuracy: 0.6302\n",
      "Epoch 9/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.4980 - binary_accuracy: 0.6327 - val_loss: 1.5469 - val_binary_accuracy: 0.6327\n",
      "Epoch 10/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.4821 - binary_accuracy: 0.6333 - val_loss: 1.5338 - val_binary_accuracy: 0.6405\n",
      "<keras.callbacks.History object at 0x00000000120AB7B8>\n",
      "82/82 [==============================] - ETA: 0s\n",
      "[1.3866125694135341, 0.6448170659018726]\n",
      "[ 2.03371971] [ 1.01182401] [-3.96232641] [ 1.04927195] [ 3.39512065] [-1.55987788] [ 1.77756938] [ 1.44149971] [-0.04338736]\n",
      "hamming_loss_train 0.5381637168141593\n",
      "hamming_loss_cv 0.5837628865979382\n",
      "hamming_loss_test 0.5533536585365854\n",
      "hamming_loss_traindataset 0.5518575851393189\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_36 (Dense)             (None, 13)                156       \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 13)                0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 13)                182       \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 13)                0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 13)                182       \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 13)                0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 13)                182       \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 13)                0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 13)                182       \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 13)                0         \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 13)                182       \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 8)                 112       \n",
      "=================================================================\n",
      "Total params: 1,178\n",
      "Trainable params: 1,178\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 226 samples, validate on 97 samples\n",
      "Epoch 1/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.7173 - binary_accuracy: 0.6040 - val_loss: 2.6990 - val_binary_accuracy: 0.6082\n",
      "Epoch 2/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.5599 - binary_accuracy: 0.6051 - val_loss: 2.6891 - val_binary_accuracy: 0.6070\n",
      "Epoch 3/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.5346 - binary_accuracy: 0.6067 - val_loss: 2.6736 - val_binary_accuracy: 0.6070\n",
      "Epoch 4/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.5138 - binary_accuracy: 0.6095 - val_loss: 2.6490 - val_binary_accuracy: 0.6070\n",
      "Epoch 5/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.5024 - binary_accuracy: 0.6123 - val_loss: 2.6253 - val_binary_accuracy: 0.6095\n",
      "Epoch 6/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.4880 - binary_accuracy: 0.6123 - val_loss: 2.6049 - val_binary_accuracy: 0.6095\n",
      "Epoch 7/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.4746 - binary_accuracy: 0.6123 - val_loss: 2.5801 - val_binary_accuracy: 0.6095\n",
      "Epoch 8/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.4668 - binary_accuracy: 0.6117 - val_loss: 2.5739 - val_binary_accuracy: 0.6121\n",
      "Epoch 9/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.4612 - binary_accuracy: 0.6134 - val_loss: 2.5565 - val_binary_accuracy: 0.6121\n",
      "Epoch 10/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.4405 - binary_accuracy: 0.6150 - val_loss: 2.5217 - val_binary_accuracy: 0.6121\n",
      "<keras.callbacks.History object at 0x0000000016CCBBA8>\n",
      "82/82 [==============================] - ETA: 0s\n",
      "[2.1307215167254938, 0.66310974446738635]\n",
      "[ 6.50334178] [ 7.93488542] [-4.1419291] [ 3.31124145] [ 7.75016884] [-0.26619256] [ 0.62389188] [-3.92530319] [-0.12380895]\n",
      "hamming_loss_train 0.39491150442477874\n",
      "hamming_loss_cv 0.38917525773195877\n",
      "hamming_loss_test 0.40853658536585363\n",
      "hamming_loss_traindataset 0.3931888544891641\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_43 (Dense)             (None, 14)                168       \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 14)                0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 14)                210       \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 14)                0         \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 14)                210       \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 14)                0         \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 14)                210       \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 14)                0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 14)                210       \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 14)                0         \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 14)                210       \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 8)                 120       \n",
      "=================================================================\n",
      "Total params: 1,338\n",
      "Trainable params: 1,338\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 226 samples, validate on 97 samples\n",
      "Epoch 1/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 3.0794 - binary_accuracy: 0.7035 - val_loss: 2.8366 - val_binary_accuracy: 0.7101\n",
      "Epoch 2/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.6884 - binary_accuracy: 0.7035 - val_loss: 2.3493 - val_binary_accuracy: 0.7101\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226/226 [==============================] - ETA: 0s - loss: 2.2102 - binary_accuracy: 0.7035 - val_loss: 1.8079 - val_binary_accuracy: 0.7101\n",
      "Epoch 4/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.7913 - binary_accuracy: 0.7041 - val_loss: 1.3306 - val_binary_accuracy: 0.7152\n",
      "Epoch 5/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.4523 - binary_accuracy: 0.7030 - val_loss: 0.9818 - val_binary_accuracy: 0.7049\n",
      "Epoch 6/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.0885 - binary_accuracy: 0.6947 - val_loss: 0.7538 - val_binary_accuracy: 0.7036\n",
      "Epoch 7/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 0.8645 - binary_accuracy: 0.7035 - val_loss: 0.6393 - val_binary_accuracy: 0.7126\n",
      "Epoch 8/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 0.7498 - binary_accuracy: 0.7030 - val_loss: 0.6137 - val_binary_accuracy: 0.7126\n",
      "Epoch 9/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 0.7078 - binary_accuracy: 0.7008 - val_loss: 0.5941 - val_binary_accuracy: 0.7204\n",
      "Epoch 10/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 0.6426 - binary_accuracy: 0.6980 - val_loss: 0.5858 - val_binary_accuracy: 0.7204\n",
      "<keras.callbacks.History object at 0x000000001771AF98>\n",
      "82/82 [==============================] - ETA: 0s\n",
      "[0.56358864830761424, 0.71493901276006933]\n",
      "[-1.65962918] [ 0.42757668] [-1.54886121] [-1.11102554] [ 1.93254667] [ 0.29137101] [ 1.35172831] [ 1.66716711] [ 0.44099466]\n",
      "hamming_loss_train 0.45298672566371684\n",
      "hamming_loss_cv 0.49871134020618557\n",
      "hamming_loss_test 0.4375\n",
      "hamming_loss_traindataset 0.46671826625387\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_50 (Dense)             (None, 15)                180       \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 15)                240       \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 15)                240       \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 15)                240       \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 15)                240       \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 15)                240       \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 8)                 128       \n",
      "=================================================================\n",
      "Total params: 1,508\n",
      "Trainable params: 1,508\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 226 samples, validate on 97 samples\n",
      "Epoch 1/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 3.1190 - binary_accuracy: 0.6250 - val_loss: 3.0327 - val_binary_accuracy: 0.6211\n",
      "Epoch 2/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.8502 - binary_accuracy: 0.6233 - val_loss: 2.7486 - val_binary_accuracy: 0.6108\n",
      "Epoch 3/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.6378 - binary_accuracy: 0.6278 - val_loss: 2.4984 - val_binary_accuracy: 0.6186\n",
      "Epoch 4/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.4748 - binary_accuracy: 0.6366 - val_loss: 2.2617 - val_binary_accuracy: 0.6237\n",
      "Epoch 5/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.2543 - binary_accuracy: 0.6388 - val_loss: 2.1378 - val_binary_accuracy: 0.6289\n",
      "Epoch 6/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.1432 - binary_accuracy: 0.6460 - val_loss: 2.0907 - val_binary_accuracy: 0.6418\n",
      "Epoch 7/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.9854 - binary_accuracy: 0.6488 - val_loss: 1.9238 - val_binary_accuracy: 0.6495\n",
      "Epoch 8/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.8022 - binary_accuracy: 0.6621 - val_loss: 1.6933 - val_binary_accuracy: 0.6469\n",
      "Epoch 9/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.6444 - binary_accuracy: 0.6643 - val_loss: 1.5616 - val_binary_accuracy: 0.6637\n",
      "Epoch 10/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.4149 - binary_accuracy: 0.6715 - val_loss: 1.3642 - val_binary_accuracy: 0.6649\n",
      "<keras.callbacks.History object at 0x0000000018F8FEB8>\n",
      "82/82 [==============================] - ETA: 0s\n",
      "[1.1426718438543924, 0.68292683944469545]\n",
      "[ 5.95084002] [-3.70138281] [-1.16379632] [ 0.97439601] [ 4.20736016] [-2.93143896] [-2.9300966] [-1.95864861] [-0.28186748]\n",
      "hamming_loss_train 0.4043141592920354\n",
      "hamming_loss_cv 0.4329896907216495\n",
      "hamming_loss_test 0.4329268292682927\n",
      "hamming_loss_traindataset 0.41292569659442724\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_57 (Dense)             (None, 16)                192       \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (None, 8)                 136       \n",
      "=================================================================\n",
      "Total params: 1,688\n",
      "Trainable params: 1,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 226 samples, validate on 97 samples\n",
      "Epoch 1/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 3.0800 - binary_accuracy: 0.6997 - val_loss: 2.3910 - val_binary_accuracy: 0.7062\n",
      "Epoch 2/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.5572 - binary_accuracy: 0.6969 - val_loss: 2.2316 - val_binary_accuracy: 0.6985\n",
      "Epoch 3/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.3418 - binary_accuracy: 0.6914 - val_loss: 2.0352 - val_binary_accuracy: 0.6920\n",
      "Epoch 4/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.0939 - binary_accuracy: 0.6764 - val_loss: 1.8495 - val_binary_accuracy: 0.6714\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226/226 [==============================] - ETA: 0s - loss: 1.8840 - binary_accuracy: 0.6643 - val_loss: 1.6678 - val_binary_accuracy: 0.6559\n",
      "Epoch 6/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.7156 - binary_accuracy: 0.6593 - val_loss: 1.6142 - val_binary_accuracy: 0.6508\n",
      "Epoch 7/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.5893 - binary_accuracy: 0.6521 - val_loss: 1.5015 - val_binary_accuracy: 0.6456\n",
      "Epoch 8/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.5062 - binary_accuracy: 0.6482 - val_loss: 1.4740 - val_binary_accuracy: 0.6482\n",
      "Epoch 9/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.4423 - binary_accuracy: 0.6460 - val_loss: 1.4387 - val_binary_accuracy: 0.6495\n",
      "Epoch 10/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.3906 - binary_accuracy: 0.6388 - val_loss: 1.4179 - val_binary_accuracy: 0.6430\n",
      "<keras.callbacks.History object at 0x0000000019BCC780>\n",
      "82/82 [==============================] - ETA: 0s\n",
      "[1.1731089615240329, 0.67073171894724781]\n",
      "[ 1.16704079] [-1.05382788] [ 0.03280007] [-5.90706639] [ 1.52116965] [-1.07878117] [-1.4893756] [-2.98675699] [ 0.09791884]\n",
      "hamming_loss_train 0.39214601769911506\n",
      "hamming_loss_cv 0.40850515463917525\n",
      "hamming_loss_test 0.39634146341463417\n",
      "hamming_loss_traindataset 0.39705882352941174\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_64 (Dense)             (None, 17)                204       \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 17)                0         \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 17)                306       \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 17)                0         \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 17)                306       \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 17)                0         \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 17)                306       \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 17)                0         \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 17)                306       \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 17)                0         \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 17)                306       \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 8)                 144       \n",
      "=================================================================\n",
      "Total params: 1,878\n",
      "Trainable params: 1,878\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 226 samples, validate on 97 samples\n",
      "Epoch 1/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.8075 - binary_accuracy: 0.6914 - val_loss: 2.5420 - val_binary_accuracy: 0.7049\n",
      "Epoch 2/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.4990 - binary_accuracy: 0.6975 - val_loss: 2.3784 - val_binary_accuracy: 0.7036\n",
      "Epoch 3/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.2348 - binary_accuracy: 0.6997 - val_loss: 1.9303 - val_binary_accuracy: 0.7023\n",
      "Epoch 4/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.6376 - binary_accuracy: 0.6991 - val_loss: 1.4594 - val_binary_accuracy: 0.6894\n",
      "Epoch 5/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.3719 - binary_accuracy: 0.6969 - val_loss: 1.3018 - val_binary_accuracy: 0.6817\n",
      "Epoch 6/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.2966 - binary_accuracy: 0.6842 - val_loss: 1.2606 - val_binary_accuracy: 0.6791\n",
      "Epoch 7/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.2329 - binary_accuracy: 0.6820 - val_loss: 1.2028 - val_binary_accuracy: 0.6817\n",
      "Epoch 8/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.1596 - binary_accuracy: 0.6925 - val_loss: 1.0618 - val_binary_accuracy: 0.6843\n",
      "Epoch 9/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.0973 - binary_accuracy: 0.7046 - val_loss: 1.0050 - val_binary_accuracy: 0.6869\n",
      "Epoch 10/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.0264 - binary_accuracy: 0.7124 - val_loss: 0.9042 - val_binary_accuracy: 0.6894\n",
      "<keras.callbacks.History object at 0x000000001B435780>\n",
      "82/82 [==============================] - ETA: 0s\n",
      "[0.77383370661154027, 0.7301829340981274]\n",
      "[-3.26171374] [ 2.04030081] [ 1.2285157] [ 0.09645782] [-1.42135659] [-0.62480848] [-2.71150764] [ 1.64716258] [ 0.94879947]\n",
      "hamming_loss_train 0.4823008849557522\n",
      "hamming_loss_cv 0.4806701030927835\n",
      "hamming_loss_test 0.4405487804878049\n",
      "hamming_loss_traindataset 0.4818111455108359\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_71 (Dense)             (None, 18)                216       \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 18)                0         \n",
      "_________________________________________________________________\n",
      "dense_72 (Dense)             (None, 18)                342       \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 18)                0         \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 18)                342       \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 18)                0         \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 18)                342       \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 18)                0         \n",
      "_________________________________________________________________\n",
      "dense_75 (Dense)             (None, 18)                342       \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 18)                0         \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 18)                342       \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 8)                 152       \n",
      "=================================================================\n",
      "Total params: 2,078\n",
      "Trainable params: 2,078\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 226 samples, validate on 97 samples\n",
      "Epoch 1/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.0980 - binary_accuracy: 0.6305 - val_loss: 1.8250 - val_binary_accuracy: 0.6714\n",
      "Epoch 2/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.9933 - binary_accuracy: 0.6521 - val_loss: 1.7915 - val_binary_accuracy: 0.6791\n",
      "Epoch 3/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.9516 - binary_accuracy: 0.6549 - val_loss: 1.7830 - val_binary_accuracy: 0.6894\n",
      "Epoch 4/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.9406 - binary_accuracy: 0.6549 - val_loss: 1.7802 - val_binary_accuracy: 0.6997\n",
      "Epoch 5/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.8874 - binary_accuracy: 0.6621 - val_loss: 1.7782 - val_binary_accuracy: 0.7062\n",
      "Epoch 6/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.8686 - binary_accuracy: 0.6659 - val_loss: 1.7534 - val_binary_accuracy: 0.7126\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226/226 [==============================] - ETA: 0s - loss: 1.8610 - binary_accuracy: 0.6781 - val_loss: 1.7339 - val_binary_accuracy: 0.7062\n",
      "Epoch 8/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.8648 - binary_accuracy: 0.6875 - val_loss: 1.7454 - val_binary_accuracy: 0.7126\n",
      "Epoch 9/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.8677 - binary_accuracy: 0.6925 - val_loss: 1.7434 - val_binary_accuracy: 0.7126\n",
      "Epoch 10/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.8542 - binary_accuracy: 0.6947 - val_loss: 1.7409 - val_binary_accuracy: 0.7191\n",
      "<keras.callbacks.History object at 0x000000001BF66CC0>\n",
      "82/82 [==============================] - ETA: 0s\n",
      "[1.6556191473472408, 0.70121951219512191]\n",
      "[ 0.29410637] [ 4.1823829] [ 2.50074849] [ 0.39934441] [ 6.42058063] [-0.91986694] [ 0.88261898] [-0.759057] [ 0.12140522]\n",
      "hamming_loss_train 0.3329646017699115\n",
      "hamming_loss_cv 0.327319587628866\n",
      "hamming_loss_test 0.3475609756097561\n",
      "hamming_loss_traindataset 0.33126934984520123\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_78 (Dense)             (None, 19)                228       \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 19)                0         \n",
      "_________________________________________________________________\n",
      "dense_79 (Dense)             (None, 19)                380       \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 19)                0         \n",
      "_________________________________________________________________\n",
      "dense_80 (Dense)             (None, 19)                380       \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 19)                0         \n",
      "_________________________________________________________________\n",
      "dense_81 (Dense)             (None, 19)                380       \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 19)                0         \n",
      "_________________________________________________________________\n",
      "dense_82 (Dense)             (None, 19)                380       \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 19)                0         \n",
      "_________________________________________________________________\n",
      "dense_83 (Dense)             (None, 19)                380       \n",
      "_________________________________________________________________\n",
      "dense_84 (Dense)             (None, 8)                 160       \n",
      "=================================================================\n",
      "Total params: 2,288\n",
      "Trainable params: 2,288\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 226 samples, validate on 97 samples\n",
      "Epoch 1/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 3.5841 - binary_accuracy: 0.5857 - val_loss: 3.0193 - val_binary_accuracy: 0.6173\n",
      "Epoch 2/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.9217 - binary_accuracy: 0.6112 - val_loss: 2.6281 - val_binary_accuracy: 0.6366\n",
      "Epoch 3/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.6241 - binary_accuracy: 0.6322 - val_loss: 2.3073 - val_binary_accuracy: 0.6688\n",
      "Epoch 4/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.4774 - binary_accuracy: 0.6421 - val_loss: 2.1382 - val_binary_accuracy: 0.6611\n",
      "Epoch 5/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.3168 - binary_accuracy: 0.6388 - val_loss: 1.9296 - val_binary_accuracy: 0.6611\n",
      "Epoch 6/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.0460 - binary_accuracy: 0.6366 - val_loss: 1.8905 - val_binary_accuracy: 0.6585\n",
      "Epoch 7/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.0146 - binary_accuracy: 0.6289 - val_loss: 1.8733 - val_binary_accuracy: 0.6521\n",
      "Epoch 8/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.9933 - binary_accuracy: 0.6250 - val_loss: 1.7921 - val_binary_accuracy: 0.6469\n",
      "Epoch 9/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.8740 - binary_accuracy: 0.6294 - val_loss: 1.7108 - val_binary_accuracy: 0.6418\n",
      "Epoch 10/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.7340 - binary_accuracy: 0.6200 - val_loss: 1.7138 - val_binary_accuracy: 0.6405\n",
      "<keras.callbacks.History object at 0x000000001C9F7CF8>\n",
      "82/82 [==============================] - ETA: 0s\n",
      "[1.5323174290540742, 0.6402438908088498]\n",
      "[ 0.11011401] [ 1.02254207] [ 0.20646874] [-0.58663818] [ 0.28081074] [-0.14578567] [-0.49566486] [ 0.72802263] [ 0.09972702]\n",
      "hamming_loss_train 0.4618362831858407\n",
      "hamming_loss_cv 0.48711340206185566\n",
      "hamming_loss_test 0.46189024390243905\n",
      "hamming_loss_traindataset 0.4694272445820433\n"
     ]
    }
   ],
   "source": [
    "y_test = y_o_test;\n",
    "Neuron = 13;\n",
    "num_classes = 8;\n",
    "for Neuron in range (10,20):\n",
    "    model = Sequential ()\n",
    "    model.add (Dense (units = Neuron, input_dim = 11))\n",
    "    model.add (layers.Activation ('tanh'))\n",
    "    model.add (Dense (units = Neuron))\n",
    "    model.add (layers.Activation ('tanh'))\n",
    "    model.add (Dense (units = Neuron))\n",
    "    model.add (layers.Activation ('tanh'))\n",
    "    model.add (Dense (units = Neuron))\n",
    "    model.add (layers.Activation ('tanh'))\n",
    "    model.add (Dense (units = Neuron))\n",
    "    model.add (layers.Activation ('tanh'))\n",
    "    model.add (Dense (units = Neuron))\n",
    "    model.add (Dense (num_classes))\n",
    "    print (model.summary())\n",
    "\n",
    "#sgd = optimizer.SGD (lr=0.01, momentum = 0, decay =0, nestrov = False)\n",
    "    model.compile (loss = 'binary_crossentropy', optimizer = 'adam', metrics=['binary_accuracy'])\n",
    "    print(model.fit(x_train, y_train, batch_size=80, verbose=1, validation_data=(x_cv, y_cv)))\n",
    "    print(model.evaluate (x_test, y_test, batch_size = 50, verbose =1))\n",
    "#help (model.evaluate)\n",
    "#this is the output units\n",
    "    y_pred_train = model.predict (x_train)\n",
    "    y_pred_cv = model.predict (x_cv)\n",
    "    y_pred_test = model.predict (x_test)\n",
    "    y_pred_whole = np.vstack ((y_pred_train, y_pred_cv)).astype(np.float32)\n",
    "    \n",
    "    #creating 100 random numbers\n",
    "    y_pred = y_pred_whole\n",
    "    f = np.genfromtxt ('C:/Users/z5023853/OneDrive - UNSW/Jupyter/Keras/f_random.csv', \n",
    "                    delimiter = ',').reshape(100,1)\n",
    "    def t(x):\n",
    "        return f.item (x)\n",
    "\n",
    "    def k (x):\n",
    "        k = np.zeros ((323,8));\n",
    "        i = 0;\n",
    "        j = 0;\n",
    "        for i in range (323):\n",
    "            for j in range (8):\n",
    "                if y_pred.item((i,j))<=t(x):\n",
    "                    np.put(k,[8*i+j],[1]);\n",
    "        return k\n",
    "\n",
    "    g = np.zeros((323,1))\n",
    "    for x in range (100):\n",
    "        m = np.sum(k(x), axis = 1).reshape(323,1);\n",
    "    #print (m.shape)\n",
    "        g = np.hstack((g,m));\n",
    "    g = np.delete(g,0,1)\n",
    "\n",
    "\n",
    "    i = 0\n",
    "    j = 0\n",
    "    m = np.zeros((323,100))\n",
    "    for i in range (323):\n",
    "        min =100\n",
    "        for j in range (100):\n",
    "            if g.item ((i,j))<=min:\n",
    "                min = g.item((i,j))\n",
    "        for j in range (100):\n",
    "            if g.item ((i,j))==min:\n",
    "                np.put(m,[100*i+j],[1])\n",
    "            \n",
    "    i = 0\n",
    "    j = 0\n",
    "    w = np.zeros((323,100))\n",
    "    for i in range (0,323):\n",
    "        for j in range (0, 100):\n",
    "            w_ = m.item((i,j))*t(j)\n",
    "            np.put(w,[100*i+j],[w_])\n",
    "\n",
    "        \n",
    "    cnt = np.count_nonzero(w, axis=1).reshape(323,1)\n",
    "    re_cnt = cnt[::-1]\n",
    "    w_total = np.sum(w, axis = 1).reshape(323,1)\n",
    "    w_avg = w_total/re_cnt\n",
    "    A = np.hstack([y_pred, np.ones((323,1))])\n",
    "    w1,w2,w3,w4,w5,w6,w7,w8,b = np.linalg.lstsq(A,w_avg)[0]\n",
    "    print (w1,w2,w3,w4,w5,w6,w7,w8,b)\n",
    "    \n",
    "    \n",
    "    def y_label (y_pred):\n",
    "    \n",
    "        I = y_pred.shape[0];\n",
    "        J = y_pred.shape[1];\n",
    "    \n",
    "        x1_Y = y_pred [:,0].reshape(I,1);\n",
    "        x2_Y = y_pred [:,1].reshape(I,1);\n",
    "        x3_Y = y_pred [:,2].reshape(I,1);\n",
    "        x4_Y = y_pred [:,3].reshape(I,1);\n",
    "        x5_Y = y_pred [:,4].reshape(I,1);\n",
    "        x6_Y = y_pred [:,5].reshape(I,1);\n",
    "        x7_Y = y_pred [:,6].reshape(I,1);\n",
    "        x8_Y = y_pred [:,7].reshape(I,1);\n",
    "        #x9_Y = y_pred [:,8].reshape(I,1);\n",
    "        #x10_Y = y_pred [:,9].reshape(I,1);\n",
    "    \n",
    "        c_x = y_pred.reshape(I,J);\n",
    "        y_thre = np.zeros((I,1));\n",
    "        y_label = np.zeros((I,J));\n",
    "    \n",
    "        i = 0;\n",
    "        j = 0;\n",
    "    \n",
    "        for i in range (I):\n",
    "            x1 = x1_Y.item(i,0);\n",
    "            x2 = x2_Y.item(i,0);\n",
    "            x3 = x3_Y.item(i,0);\n",
    "            x4 = x4_Y.item(i,0);\n",
    "            x5 = x5_Y.item(i,0);\n",
    "            x6 = x6_Y.item(i,0);\n",
    "            x7 = x7_Y.item(i,0);\n",
    "            x8 = x8_Y.item(i,0);\n",
    "            #x9 = x9_Y.item(i,0);\n",
    "            #x10 = x10_Y.item(i,0);\n",
    "            \n",
    "            y_tx = w1*x1+w2*x2+w3*x3+w4*x4+w5*x5+w6*x6+w7*x7+w8*x8+b#w9*x9+b+w10*x10;\n",
    "            np.put (y_thre, [i], [y_tx]);\n",
    "        \n",
    "        for i in range (I):\n",
    "            for j in range (J):\n",
    "                threshold = y_thre.item(i,0);\n",
    "                if c_x.item(i,j) >= threshold:\n",
    "                    np.put(y_label, [J*i+j], [1])\n",
    "    \n",
    "        return (y_label)    \n",
    "    \n",
    "    \n",
    "    y_train_label = y_label(y_pred_train)\n",
    "    y_cv_label = y_label(y_pred_cv)\n",
    "    y_test_label = y_label(y_pred_test)\n",
    "    y_t_label = np.vstack((y_train_label,y_cv_label))\n",
    "    y_t = np.vstack((y_train,y_cv))\n",
    "\n",
    "    print('hamming_loss_train',hamming_loss(y_train, y_train_label))\n",
    "    print('hamming_loss_cv',hamming_loss(y_cv, y_cv_label))\n",
    "    print('hamming_loss_test',hamming_loss(y_test, y_test_label))\n",
    "    print('hamming_loss_traindataset',hamming_loss(y_t, y_t_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 226 samples, validate on 97 samples\n",
      "Epoch 1/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.3333 - binary_accuracy: 0.6538 - val_loss: 2.0051 - val_binary_accuracy: 0.6534\n",
      "Epoch 2/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 2.0015 - binary_accuracy: 0.6632 - val_loss: 1.7838 - val_binary_accuracy: 0.6611\n",
      "Epoch 3/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.7868 - binary_accuracy: 0.6704 - val_loss: 1.6402 - val_binary_accuracy: 0.6791\n",
      "Epoch 4/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.5706 - binary_accuracy: 0.6759 - val_loss: 1.5486 - val_binary_accuracy: 0.6765\n",
      "Epoch 5/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.4341 - binary_accuracy: 0.6798 - val_loss: 1.4912 - val_binary_accuracy: 0.6753\n",
      "Epoch 6/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.3810 - binary_accuracy: 0.6759 - val_loss: 1.4380 - val_binary_accuracy: 0.6843\n",
      "Epoch 7/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.3467 - binary_accuracy: 0.6803 - val_loss: 1.3761 - val_binary_accuracy: 0.6830\n",
      "Epoch 8/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.3118 - binary_accuracy: 0.6881 - val_loss: 1.3137 - val_binary_accuracy: 0.6856\n",
      "Epoch 9/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.2752 - binary_accuracy: 0.6914 - val_loss: 1.2191 - val_binary_accuracy: 0.6985\n",
      "Epoch 10/10\n",
      "226/226 [==============================] - ETA: 0s - loss: 1.2348 - binary_accuracy: 0.6914 - val_loss: 1.1747 - val_binary_accuracy: 0.6997\n",
      "<keras.callbacks.History object at 0x000000001D454DA0>\n",
      "82/82 [==============================] - ETA: 0s\n",
      "[1.1762420113493757, 0.71646339718888441]\n",
      "[-2.23891382] [ 0.45035306] [ 0.02820088] [ 3.14668741] [-1.03685195] [-0.15094447] [ 0.62262412] [ 3.42496801] [ 0.27416683]\n",
      "hamming_loss_train 0.3506637168141593\n",
      "hamming_loss_cv 0.33505154639175255\n",
      "hamming_loss_test 0.2896341463414634\n",
      "hamming_loss_traindataset 0.3459752321981424\n"
     ]
    }
   ],
   "source": [
    "Neuron = 15;\n",
    "num_classes = 8;\n",
    "model = Sequential ()\n",
    "model.add (Dense (units = Neuron, input_dim = 11))\n",
    "model.add (layers.Activation ('tanh'))\n",
    "model.add (Dense (units = Neuron))\n",
    "model.add (layers.Activation ('tanh'))\n",
    "model.add (Dense (units = Neuron))\n",
    "model.add (layers.Activation ('tanh'))\n",
    "model.add (Dense (units = Neuron))\n",
    "model.add (Dense(num_classes))\n",
    "#print (model.summary())\n",
    "\n",
    "#sgd = optimizer.SGD (lr=0.01, momentum = 0, decay =0, nestrov = False)\n",
    "model.compile (loss = 'binary_crossentropy', optimizer = 'adam', metrics=['binary_accuracy'])\n",
    "print(model.fit(x_train, y_train,batch_size=200, verbose=1, validation_data=(x_cv, y_cv)))\n",
    "print(model.evaluate (x_test, y_test, batch_size = 50, verbose =1))\n",
    "#help (model.evaluate)\n",
    "#this is the output units\n",
    "y_pred_train = model.predict (x_train)\n",
    "y_pred_cv = model.predict (x_cv)\n",
    "y_pred_test = model.predict (x_test)\n",
    "y_pred_whole = np.vstack ((y_pred_train, y_pred_cv)).astype(np.float32)\n",
    "    \n",
    "#creating 100 random numbers\n",
    "y_pred = y_pred_whole\n",
    "f = np.genfromtxt ('C:/Users/z5023853/OneDrive - UNSW/Jupyter/Keras/f_random.csv', \n",
    "                    delimiter = ',').reshape(100,1)\n",
    "def t(x):\n",
    "    return f.item (x)\n",
    "\n",
    "def k (x):\n",
    "    k = np.zeros ((323,8));\n",
    "    i = 0;\n",
    "    j = 0;\n",
    "    for i in range (323):\n",
    "        for j in range (8):\n",
    "            if y_pred.item((i,j))<=t(x):\n",
    "                np.put(k,[8*i+j],[1]);\n",
    "    return k\n",
    "\n",
    "g = np.zeros((323,1))\n",
    "for x in range (100):\n",
    "    m = np.sum(k(x), axis = 1).reshape(323,1);\n",
    "    #print (m.shape)\n",
    "    g = np.hstack((g,m));\n",
    "g = np.delete(g,0,1)\n",
    "\n",
    "\n",
    "i = 0\n",
    "j = 0\n",
    "m = np.zeros((323,100))\n",
    "for i in range (323):\n",
    "    min =100\n",
    "    for j in range (100):\n",
    "        if g.item ((i,j))<=min:\n",
    "            min = g.item((i,j))\n",
    "    for j in range (100):\n",
    "        if g.item ((i,j))==min:\n",
    "            np.put(m,[100*i+j],[1])\n",
    "            \n",
    "i = 0\n",
    "j = 0\n",
    "w = np.zeros((323,100))\n",
    "for i in range (0,323):\n",
    "    for j in range (0, 100):\n",
    "        w_ = m.item((i,j))*t(j)\n",
    "        np.put(w,[100*i+j],[w_])\n",
    "\n",
    "        \n",
    "cnt = np.count_nonzero(w, axis=1).reshape(323,1)\n",
    "re_cnt = cnt[::-1]\n",
    "w_total = np.sum(w, axis = 1).reshape(323,1)\n",
    "w_avg = w_total/re_cnt\n",
    "A = np.hstack([y_pred, np.ones((323,1))])\n",
    "w1,w2,w3,w4,w5,w6,w7,w8,b = np.linalg.lstsq(A,w_avg)[0]\n",
    "print (w1,w2,w3,w4,w5,w6,w7,w8,b)\n",
    "    \n",
    "    \n",
    "def y_label (y_pred):\n",
    "    \n",
    "    I = y_pred.shape[0];\n",
    "    J = y_pred.shape[1];\n",
    "    \n",
    "    x1_Y = y_pred [:,0].reshape(I,1);\n",
    "    x2_Y = y_pred [:,1].reshape(I,1);\n",
    "    x3_Y = y_pred [:,2].reshape(I,1);\n",
    "    x4_Y = y_pred [:,3].reshape(I,1);\n",
    "    x5_Y = y_pred [:,4].reshape(I,1);\n",
    "    x6_Y = y_pred [:,5].reshape(I,1);\n",
    "    x7_Y = y_pred [:,6].reshape(I,1);\n",
    "    x8_Y = y_pred [:,7].reshape(I,1);\n",
    "        #x9_Y = y_pred [:,8].reshape(I,1);\n",
    "        #x10_Y = y_pred [:,9].reshape(I,1);\n",
    "    \n",
    "    c_x = y_pred.reshape(I,J);\n",
    "    y_thre = np.zeros((I,1));\n",
    "    y_label = np.zeros((I,J));\n",
    "    \n",
    "    i = 0;\n",
    "    j = 0;\n",
    "    \n",
    "    for i in range (I):\n",
    "        x1 = x1_Y.item(i,0);\n",
    "        x2 = x2_Y.item(i,0);\n",
    "        x3 = x3_Y.item(i,0);\n",
    "        x4 = x4_Y.item(i,0);\n",
    "        x5 = x5_Y.item(i,0);\n",
    "        x6 = x6_Y.item(i,0);\n",
    "        x7 = x7_Y.item(i,0);\n",
    "        x8 = x8_Y.item(i,0);\n",
    "            #x9 = x9_Y.item(i,0);\n",
    "            #x10 = x10_Y.item(i,0);\n",
    "            \n",
    "        y_tx = w1*x1+w2*x2+w3*x3+w4*x4+w5*x5+w6*x6+w7*x7+w8*x8+b#w9*x9+b+w10*x10;\n",
    "        np.put (y_thre, [i], [y_tx]);\n",
    "        \n",
    "    for i in range (I):\n",
    "        for j in range (J):\n",
    "            threshold = y_thre.item(i,0);\n",
    "            if c_x.item(i,j) >= threshold:\n",
    "                np.put(y_label, [J*i+j], [1])\n",
    "    \n",
    "    return (y_label)    \n",
    "    \n",
    "    \n",
    "y_train_label = y_label(y_pred_train)\n",
    "y_cv_label = y_label(y_pred_cv)\n",
    "y_test_label = y_label(y_pred_test)\n",
    "y_t_label = np.vstack((y_train_label,y_cv_label))\n",
    "y_t = np.vstack((y_train,y_cv))\n",
    "\n",
    "print('hamming_loss_train',hamming_loss(y_train, y_train_label))\n",
    "print('hamming_loss_cv',hamming_loss(y_cv, y_cv_label))\n",
    "print('hamming_loss_test',hamming_loss(y_test, y_test_label))\n",
    "print('hamming_loss_traindataset',hamming_loss(y_t, y_t_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "#save model to json, weights to HDF5\n",
    "model_json = model.to_json ()\n",
    "with open ('trainC.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights('trainC.h5')\n",
    "print ('Saved model to disk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
